{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "team2_TrafficLabellingClean.py\n",
    "-------------------------------\n",
    "Functions:\n",
    "- Safe CSV reader (utf-8-sig with latin1 fallback).\n",
    "- Basic cleaning: dropna, duplicates, constant columns,\n",
    "  IP/Timestamp columns, extreme values.\n",
    "- Lightweight feature selection: low variance filter +\n",
    "  high correlation filter.\n",
    "- Numeric optimization: downcast + rounding to reduce file size.\n",
    "- Outputs both cleaned CSV and validation report directly into /datasets.\n",
    "\"\"\"\n",
    "import mlflow\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67312606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Paths ----------------\n",
    "# Source dataset folder (update this path if needed)\n",
    "DATA_FOLDER = r\"C:\\Users\\hi\\AI-CloudSec-System\\data\\traffic\"\n",
    "\n",
    "# ---------------- Corrected Path Definitions ----------------\n",
    "# When running in Jupyter, os.getcwd() gives the notebook directory. \n",
    "# We navigate up one level (os.pardir) to the project root (AI-CloudSec-System-1).\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) \n",
    "# The target output directory is Step1-Datasets-Feature-Engineering.\n",
    "OUT_DIR = os.path.join(PROJECT_DIR, \"Step1-Datasets-Feature-Engineering\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define the final output file paths\n",
    "OUT_FILE = os.path.join(OUT_DIR, \"team2_TrafficLabellingClean.csv\")\n",
    "REPORT = os.path.join(OUT_DIR, \"team2_TrafficLabellingClean_report.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Safe CSV Reader ----------------\n",
    "def safe_read_csv(path):\n",
    "    try:\n",
    "        print(f\"Reading {path} with utf-8-sig ...\")\n",
    "        return pd.read_csv(path, low_memory=False, encoding=\"utf-8-sig\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"⚠️ UTF-8 failed for {path}, retrying with latin1 ...\")\n",
    "        return pd.read_csv(path, low_memory=False, encoding=\"latin1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------- Cleaning ----------------\n",
    "def clean_dataframe(df, log):\n",
    "    before = len(df)\n",
    "    df = df.dropna().drop_duplicates()\n",
    "    log.append(\n",
    "        f\"Dropna + duplicates: {before - len(df)} rows removed, now {len(df)} rows\"\n",
    "    )\n",
    "\n",
    "    const_cols = df.columns[df.nunique() <= 1].tolist()\n",
    "    if const_cols:\n",
    "        df = df.drop(columns=const_cols)\n",
    "        log.append(f\"Dropped {len(const_cols)} constant cols: {const_cols}\")\n",
    "\n",
    "    drop_cols = [c for c in df.columns if \"IP\" in c or \"Timestamp\" in c]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "        log.append(f\"Dropped {len(drop_cols)} IP/Timestamp cols\")\n",
    "\n",
    "    if \" Flow Duration\" in df.columns:\n",
    "        df = df[(df[\" Flow Duration\"] > 0) & (df[\" Flow Duration\"] < 3600)]\n",
    "    if \" Flow Bytes/s\" in df.columns:\n",
    "        df = df[df[\" Flow Bytes/s\"] < 1e9]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee0833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Feature Selection ----------------\n",
    "def feature_selection(df, log, label_col=\" Label\"):\n",
    "    if label_col in df.columns:\n",
    "        X = df.drop(columns=[label_col], errors=\"ignore\")\n",
    "    else:\n",
    "        X = df\n",
    "\n",
    "    X = X.select_dtypes(include=np.number)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    X_var = selector.fit_transform(X_scaled)\n",
    "    kept_cols = X.columns[selector.get_support()]\n",
    "    log.append(f\"Low variance removed: {X.shape[1] - len(kept_cols)} cols\")\n",
    "\n",
    "    X_df = pd.DataFrame(X_var, columns=kept_cols)\n",
    "    corr = X_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "    if to_drop:\n",
    "        X_df = X_df.drop(columns=to_drop)\n",
    "        log.append(f\"High correlation removed: {len(to_drop)} cols\")\n",
    "\n",
    "    if label_col in df.columns:\n",
    "        X_df[label_col] = df[label_col].values\n",
    "\n",
    "    return X_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Numeric Optimization ----------------\n",
    "def optimize_numeric(df, log, decimals=2):\n",
    "    before_mem = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if pd.api.types.is_integer_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "        else:\n",
    "            df[col] = df[col].round(decimals)\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "    after_mem = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    ratio = (before_mem - after_mem) / before_mem * 100\n",
    "    log.append(\n",
    "        f\"Optimized numeric cols: {before_mem:.2f}MB →\"\n",
    "        f\"{after_mem:.2f}MB (↓{ratio:.1f}%)\"\n",
    "    )\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "338f66a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/17 13:39:36 INFO mlflow.tracking.fluent: Experiment with name 'Team2_Feature_Engineering_Traffic_Data' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\hi\\\\AI-CloudSec-System\\\\data\\\\traffic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_cleaning_and_feature_selection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# --- [Start my origin code logical] ---\u001b[39;00m\n\u001b[1;32m     13\u001b[0m files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_FOLDER, f)\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_FOLDER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     18\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [safe_read_csv(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\hi\\\\AI-CloudSec-System\\\\data\\\\traffic'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- Main (MLflow Integrated) ----------------\n",
    "log = []\n",
    "\n",
    "# 1. Set MLflow Lab Name\n",
    "mlflow.set_experiment(\"Team2_Feature_Engineering_Traffic_Data\")\n",
    "\n",
    "# 2. Run a new MLflow \n",
    "with mlflow.start_run() as run:\n",
    "    # Log the type of operation/step\n",
    "    mlflow.set_tag(\"step\", \"data_cleaning_and_feature_selection\")\n",
    "\n",
    "    # --- [Start my origin code logical] ---\n",
    "    files = [\n",
    "        os.path.join(DATA_FOLDER, f)\n",
    "        for f in os.listdir(DATA_FOLDER)\n",
    "        if f.endswith(\".csv\")\n",
    "    ]\n",
    "    dfs = [safe_read_csv(f) for f in files]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    log.append(f\"Merged {len(files)} files: {df.shape}\")\n",
    "\n",
    "    df = clean_dataframe(df, log)\n",
    "    df_final = feature_selection(df, log)\n",
    "    df_final = optimize_numeric(df_final, log, decimals=2)\n",
    "\n",
    "    # record key index\n",
    "    mlflow.log_metric(\"final_rows\", len(df_final))\n",
    "    mlflow.log_metric(\"final_columns\", df_final.shape[1])\n",
    "\n",
    "    # save printout(original code)\n",
    "    # Note: OUT_FILE & REPORT Patch should be capable to visit in Notebook\n",
    "    df_final.to_csv(OUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "    with open(REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(str(x) for x in log))\n",
    "\n",
    "    # 3. Register MLflow Artifacts files\n",
    "    mlflow.log_artifact(OUT_FILE, artifact_path=\"cleaned_data\")\n",
    "    mlflow.log_artifact(REPORT, artifact_path=\"reports\")\n",
    "\n",
    "    print(\"✅ Saved cleaned dataset:\", OUT_FILE, df_final.shape)\n",
    "    print(\"📊 Validation report written:\", REPORT)\n",
    "    print(f\"MLflow Run ID: {run.info.run_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
