{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 1. Imports and Setup\n",
    "\"\"\"\n",
    "team2_TrafficLabellingClean.ipynb\n",
    "----------------------------------\n",
    "Step 1: Dataset Cleaning and Optimization (Team 2)\n",
    "----------------------------------\n",
    "Pipeline:\n",
    "- Read 8 raw CSVs (CICIDS2017)\n",
    "- Cleaning: dropna, duplicates, constant columns, IP/Timestamp\n",
    "- Feature selection: low-variance + high-correlation filters\n",
    "- Numeric optimization: downcast to reduce memory footprint\n",
    "- Save cleaned CSV under ./team2_TrafficLabelling/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Detect notebook folder\n",
    "BASE_DIR = os.path.dirname(__file__) if \"__file__\" in globals() else os.getcwd()\n",
    "\n",
    "# ✅ 自动检测数据路径\n",
    "possible_dirs = [\n",
    "    os.path.normpath(os.path.join(BASE_DIR, \"..\", \"data\", \"traffic\")),\n",
    "    r\"C:\\Users\\hi\\AI-CloudSec-System\\data\\traffic\",   # 你旧仓库的真实路径\n",
    "]\n",
    "DATA_FOLDER = None\n",
    "for p in possible_dirs:\n",
    "    if os.path.exists(p):\n",
    "        DATA_FOLDER = p\n",
    "        break\n",
    "\n",
    "if DATA_FOLDER is None:\n",
    "    raise FileNotFoundError(\"❌ Could not find the traffic dataset folder. Please check the path manually.\")\n",
    "\n",
    "# 输出检查信息\n",
    "print(\"Environment initialized.\")\n",
    "print(\"BASE_DIR     :\", BASE_DIR)\n",
    "print(\"DATA_FOLDER  :\", DATA_FOLDER)\n",
    "print(\"Files inside :\", len(os.listdir(DATA_FOLDER)))\n",
    "\n",
    "# 输出目录下前几个文件名\n",
    "for f in os.listdir(DATA_FOLDER)[:5]:\n",
    "    print(\"  -\", f)\n",
    "\n",
    "# 输出路径设置\n",
    "OUTPUT_DIR  = os.path.join(BASE_DIR, \"team2_TrafficLabelling\")\n",
    "OUTPUT_NAME = \"team2_TrafficLabellingClean.csv\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_DIR, OUTPUT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 2. Load Raw Data\n",
    "raw_files = [\n",
    "    \"Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"Wednesday-workingHours.pcap_ISCX.csv\",\n",
    "    \"Thursday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"Friday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "    \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
    "    \"Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
    "]\n",
    "\n",
    "dfs, missing, failed = [], [], []\n",
    "for fname in raw_files:\n",
    "    fpath = os.path.join(DATA_FOLDER, fname)\n",
    "    if not os.path.exists(fpath):\n",
    "        missing.append(fname)\n",
    "        continue\n",
    "    try:\n",
    "        df_tmp = pd.read_csv(fpath, encoding=\"utf-8-sig\", low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        df_tmp = pd.read_csv(fpath, encoding=\"latin1\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        failed.append((fname, str(e)))\n",
    "        continue\n",
    "    dfs.append(df_tmp)\n",
    "\n",
    "assert dfs, f\"No files loaded. Missing={missing}, Failed={failed}\"\n",
    "if missing: print(\"⚠ Missing files:\", missing)\n",
    "if failed:  print(\"⚠ Failed to read:\", failed)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Merged dataset shape:\", df.shape)\n",
    "assert df.shape[0] > 0 and df.shape[1] > 0, \"Merged dataframe is empty.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3. Basic Cleaning\n",
    "before = df.shape\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "after = df.shape\n",
    "print(f\"Dropna+duplicates: {before} -> {after} | removed={before[0]-after[0]}\")\n",
    "assert df.shape[0] > 0, \"All rows removed after cleaning; check inputs/thresholds.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 4. Feature Removal (constant, IP/Timestamp, low variance, high correlation)\n",
    "# 4.1 constants (zero variance)\n",
    "constant_cols = [\n",
    "    \"Bwd PSH Flags\", \"Bwd URG Flags\",\n",
    "    \"Fwd Avg Bytes/Bulk\", \"Fwd Avg Packets/Bulk\", \"Fwd Avg Bulk Rate\",\n",
    "    \"Bwd Avg Bytes/Bulk\", \"Bwd Avg Packets/Bulk\", \"Bwd Avg Bulk Rate\",\n",
    "]\n",
    "const_in_df = [c for c in constant_cols if c in df.columns]\n",
    "df.drop(columns=const_in_df, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# 4.2 IP / Timestamp identifiers\n",
    "id_like = [c for c in df.columns if (\"IP\" in c) or (\"Timestamp\" in c)]\n",
    "df.drop(columns=id_like, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# 4.3 low variance (numeric only)\n",
    "low_var_cols = []\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "if not numeric_df.empty:\n",
    "    sel = VarianceThreshold(threshold=1e-4)\n",
    "    sel.fit(numeric_df)\n",
    "    low_var_cols = list(numeric_df.columns[~sel.get_support()])\n",
    "    if low_var_cols:\n",
    "        df.drop(columns=low_var_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# 4.4 high correlation (numeric only)\n",
    "corr_drop = []\n",
    "num_only = df.select_dtypes(include=[np.number])\n",
    "if not num_only.empty:\n",
    "    corr = num_only.corr()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    corr_drop = [c for c in upper.columns if any(upper[c].abs() > 0.95)]\n",
    "    if corr_drop:\n",
    "        df.drop(columns=corr_drop, inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(\"Dropped (constants):\", len(const_in_df), \"|\", const_in_df)\n",
    "print(\"Dropped (id-like)  :\", len(id_like))\n",
    "print(\"Dropped (low-var)  :\", len(low_var_cols))\n",
    "print(\"Dropped (high-corr):\", len(corr_drop))\n",
    "print(\"Current shape      :\", df.shape)\n",
    "assert df.shape[1] > 0, \"All columns removed; relax thresholds or review features.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 5. Optimize Numeric Types\n",
    "for col in df.select_dtypes(include=[\"float64\"]).columns:\n",
    "    df[col] = df[col].astype(\"float32\")\n",
    "for col in df.select_dtypes(include=[\"int64\"]).columns:\n",
    "    df[col] = df[col].astype(\"int32\")\n",
    "print(\"Optimized numeric dtypes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 6. Save and Validate\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "assert df.shape[0] > 0 and df.shape[1] > 0, \"Empty dataframe at save time.\"\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\", lineterminator=\"\\n\")\n",
    "print(f\"✅ Cleaned dataset saved to: {OUTPUT_PATH}\")\n",
    "print(\"Final shape:\", df.shape)\n",
    "\n",
    "assert os.path.exists(OUTPUT_PATH), \"CSV was not written.\"\n",
    "size_mb = round(os.path.getsize(OUTPUT_PATH)/(1024**2), 2)\n",
    "print(\"File size (MB):\", size_mb)\n",
    "\n",
    "# quick preview\n",
    "try:\n",
    "    preview = pd.read_csv(OUTPUT_PATH, nrows=5)\n",
    "    print(\"Preview shape:\", preview.shape, \"| Columns:\", len(preview.columns))\n",
    "except Exception as e:\n",
    "    print(\"Read-back failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
