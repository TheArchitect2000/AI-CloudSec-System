{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b51852",
   "metadata": {
    "papermill": {
     "duration": 11.815195,
     "end_time": "2025-01-10T13:52:19.028978",
     "exception": false,
     "start_time": "2025-01-10T13:52:07.213783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd  # Import pandas library for data manipulation and analysis\n",
    "import numpy as np  # Import numpy for numerical operations\n",
    "import matplotlib.pyplot as plt  # Import matplotlib for plotting graphs\n",
    "import seaborn as sns  # Import seaborn for advanced data visualization\n",
    "import pickle  # Import pickle to save and load Python objects (like trained models)\n",
    "import tensorflow as tf  # Import TensorFlow for building and training deep learning models\n",
    "from tensorflow.keras.models import Sequential  # Import Sequential model class from Keras for building neural networks\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout  # Import Conv1D layer for 1D convolution (useful for sequential data like time series)\n",
    "from sklearn import preprocessing  # Import preprocessing utilities from scikit-learn\n",
    "from sklearn.model_selection import train_test_split  # Import function to split dataset into training and testing sets\n",
    "from sklearn.preprocessing import StandardScaler  # Import preprocessing utilities from scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier for building a Random Forest model\n",
    "from sklearn.model_selection import GridSearchCV  # Import GridSearchCV for hyperparameter tuning\n",
    "from sklearn.model_selection import cross_val_score  # Import cross_val_score for model evaluation using cross-validation\n",
    "from sklearn.neural_network import MLPClassifier  # Import MLPClassifier for building a Multi-Layer Perceptron model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc, precision_recall_curve  # Import accuracy_score to calculate accuracy of predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368164b",
   "metadata": {
    "papermill": {
     "duration": 94.207466,
     "end_time": "2025-01-10T13:53:53.243349",
     "exception": false,
     "start_time": "2025-01-10T13:52:19.035883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load datasets (Make sure the CSV file is in the same directory as this script. you can change the path if needed and use the v2 dataset)\n",
    "\n",
    "botnet_df_v2 = pd.read_csv('BotNeTIoT-L01-v2.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b70583",
   "metadata": {
    "papermill": {
     "duration": 0.683975,
     "end_time": "2025-01-10T13:53:53.933898",
     "exception": false,
     "start_time": "2025-01-10T13:53:53.249923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# botnet_df_v2 features\n",
    "print('botnet_df_v2 features:')   \n",
    "print(botnet_df_v2.columns)   \n",
    "print('\\n')   \n",
    "print('*'*50)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b5085",
   "metadata": {
    "papermill": {
     "duration": 16.963019,
     "end_time": "2025-01-10T13:54:10.903071",
     "exception": false,
     "start_time": "2025-01-10T13:53:53.940052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('-'*10, 'HEAD', '-'*10)   \n",
    "print(botnet_df_v2.head())   \n",
    "print('\\n')   \n",
    "\n",
    "print('-'*10, 'DESCRIBE', '-'*10)   \n",
    "print(botnet_df_v2.describe())   \n",
    "print('\\n')   \n",
    "\n",
    "print('-'*10, 'INFO', '-'*10)   \n",
    "print(botnet_df_v2.info())   \n",
    "print('\\n')   \n",
    "\n",
    "print('-'*10, 'MISSING VALUES', '-'*10)   \n",
    "print(botnet_df_v2.isnull().sum())   \n",
    "print('\\n')   \n",
    "\n",
    "print('-'*10, 'DATA TYPES', '-'*10)   \n",
    "print(botnet_df_v2.dtypes)   \n",
    "print('\\n')   \n",
    "\n",
    "# Check for unique values\n",
    "print('-'*10, 'UNIQUE VALUES', '-'*10)   \n",
    "print(botnet_df_v2.nunique())   \n",
    "print('\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f36754",
   "metadata": {
    "papermill": {
     "duration": 2.038525,
     "end_time": "2025-01-10T13:54:12.948801",
     "exception": false,
     "start_time": "2025-01-10T13:54:10.910276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display Unique values for attack and attack_subtype\n",
    "print('-'*10, 'UNIQUE VALUES FOR ATTACK AND ATTACK_SUBTYPE', '-'*10)   \n",
    "print(botnet_df_v2['Attack'].unique())   \n",
    "print(botnet_df_v2['Attack_subType'].unique())   \n",
    "print('\\n')   \n",
    "\n",
    "# Pie chart for attack and device_name\n",
    "print('-'*10, 'PIE CHART FOR ATTACK AND DEVICE_NAME', '-'*10)   \n",
    "plt.figure(figsize=(10, 10))  \n",
    "plt.subplot(2, 1, 1)  \n",
    "botnet_df_v2['Attack'].value_counts().plot.pie(autopct='%1.1f%%')  \n",
    "plt.title('Attack')  \n",
    "plt.subplot(2, 1, 2)  \n",
    "botnet_df_v2['Device_Name'].value_counts().plot.pie(autopct='%1.1f%%')  \n",
    "plt.title('Device_Name')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2556e5",
   "metadata": {
    "papermill": {
     "duration": 0.008753,
     "end_time": "2025-01-10T13:54:12.967047",
     "exception": false,
     "start_time": "2025-01-10T13:54:12.958294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Before proceeding to the correlation analysis, we need to convert the categorical features to numerical for better processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a51df",
   "metadata": {
    "papermill": {
     "duration": 0.028496,
     "end_time": "2025-01-10T13:54:13.004192",
     "exception": false,
     "start_time": "2025-01-10T13:54:12.975696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load a sample of 100,000 rows for quicker processing during development. If you have enough memory, you can load the full dataset by commenting out this line.\n",
    "botnet_df_v2 = pd.read_csv('BotNeTIoT-L01-v2.csv')   \n",
    "df = botnet_df_v2.sample(n=100000, random_state=1)# Sample 100,000 rows for quicker processing during development\n",
    "print(df.columns)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c332b99",
   "metadata": {
    "papermill": {
     "duration": 0.00941,
     "end_time": "2025-01-10T13:54:13.022463",
     "exception": false,
     "start_time": "2025-01-10T13:54:13.013053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47804cc0",
   "metadata": {
    "papermill": {
     "duration": 0.026013,
     "end_time": "2025-01-10T13:54:13.057327",
     "exception": false,
     "start_time": "2025-01-10T13:54:13.031314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert Device name to numerical\n",
    "le = preprocessing.LabelEncoder()  \n",
    "df['Device_Name'] = le.fit_transform(df['Device_Name'])  \n",
    "df['Attack'] = le.fit_transform(df['Attack'])  \n",
    "df['Attack_subType'] = le.fit_transform(df['Attack_subType'])  \n",
    "\n",
    "print('-'*10, 'DATA TYPES', '-'*10)   \n",
    "print(df.dtypes)   \n",
    "print('\\n')   \n",
    "\n",
    "print('Device_Name:', df['Device_Name'].unique())   \n",
    "print('Attack:', df['Attack'].unique())   \n",
    "print('Attack_subType:', df['Attack_subType'].unique())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b4796",
   "metadata": {
    "papermill": {
     "duration": 0.008402,
     "end_time": "2025-01-10T13:54:13.075097",
     "exception": false,
     "start_time": "2025-01-10T13:54:13.066695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f840a4e",
   "metadata": {
    "papermill": {
     "duration": 0.440558,
     "end_time": "2025-01-10T13:54:13.525741",
     "exception": false,
     "start_time": "2025-01-10T13:54:13.085183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot 'Attack' vs 'label'\n",
    "sns.countplot(x='Attack', hue='label', data=df)  \n",
    "plt.title('Attack vs Label')  \n",
    "plt.show()  \n",
    "\n",
    "# Plot 'Attack_subType' vs 'label'\n",
    "sns.countplot(x='Attack_subType', hue='label', data=df)  \n",
    "plt.title('Attack_subType vs Label')  \n",
    "plt.show()  \n",
    "\n",
    "# Drop attack and attack subtype\n",
    "df = df.drop(['Attack', 'Attack_subType'], axis=1)  \n",
    "\n",
    "print('-'*10, 'Data Types', '-'*10)   \n",
    "print(df.dtypes)   \n",
    "\n",
    "# Save the data\n",
    "df.to_csv('BoTNeTIoT-L01-v2-prepared.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910bceb",
   "metadata": {
    "papermill": {
     "duration": 0.010201,
     "end_time": "2025-01-10T13:54:13.546975",
     "exception": false,
     "start_time": "2025-01-10T13:54:13.536774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Attack and Attack_subtype are strongly correlated to the label so they should be dropped as they artificially inflate model performance without generalizing to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113c9d0",
   "metadata": {
    "papermill": {
     "duration": 1.235529,
     "end_time": "2025-01-10T13:54:14.792736",
     "exception": false,
     "start_time": "2025-01-10T13:54:13.557207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('BoTNeTIoT-L01-v2-prepared.csv')  \n",
    "\n",
    "def drop_highly_correlated_features(df, threshold=0.95):  \n",
    "    df_copy = df.copy()  \n",
    "    corr_matrix = df_copy.corr().abs()  \n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))  \n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]  \n",
    "    df_copy.drop(columns=to_drop, inplace=True)  \n",
    "    return df_copy, corr_matrix, to_drop  \n",
    "\n",
    "df, corr, dropped_features = drop_highly_correlated_features(df)  \n",
    "\n",
    "print(\"Dropped features:\", dropped_features)   \n",
    "print(\"Remaining features:\", df.columns.tolist())   \n",
    "\n",
    "corr_after = df.corr().abs()  \n",
    "plt.figure(figsize=(30, 30))  \n",
    "sns.heatmap(corr_after, annot=True, cmap=\"coolwarm\")  \n",
    "plt.title('Correlation Heatmap (After Feature Removal)')  \n",
    "plt.show()  \n",
    "\n",
    "# print the correlation matrix\n",
    "print(corr_after)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc9ce8",
   "metadata": {
    "papermill": {
     "duration": 0.015116,
     "end_time": "2025-01-10T13:54:14.823995",
     "exception": false,
     "start_time": "2025-01-10T13:54:14.808879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Saving the highest correlation pair to later visualize the data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0eb35e",
   "metadata": {
    "papermill": {
     "duration": 0.0161,
     "end_time": "2025-01-10T13:54:14.855906",
     "exception": false,
     "start_time": "2025-01-10T13:54:14.839806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Feature and Target Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95bcf8",
   "metadata": {
    "papermill": {
     "duration": 0.015129,
     "end_time": "2025-01-10T13:54:14.887395",
     "exception": false,
     "start_time": "2025-01-10T13:54:14.872266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Label Feature as Target\n",
    "\n",
    "0: Malicious behavior (attack)\n",
    "When the Attack column has values like mirai, gafgyt, and specific subtypes like ack or tcp.\n",
    "\n",
    "1: Normal behavior\n",
    "When the Attack and Attack_subType columns indicate Normal.\n",
    "\n",
    "The label feature serves as the final determination of whether the data corresponds to an attack or normal behavior. This makes it an ideal target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9e2c2",
   "metadata": {
    "papermill": {
     "duration": 0.02837,
     "end_time": "2025-01-10T13:54:14.931616",
     "exception": false,
     "start_time": "2025-01-10T13:54:14.903246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = df.drop(columns=['label'])  \n",
    "target = df['label']  \n",
    "\n",
    "print('-'*10, 'FEATURES AND TARGET', '-'*10)   \n",
    "print('Features:', features.columns.tolist())   \n",
    "print('Target:', target.name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf03d1b",
   "metadata": {
    "papermill": {
     "duration": 0.015366,
     "end_time": "2025-01-10T13:54:14.962786",
     "exception": false,
     "start_time": "2025-01-10T13:54:14.947420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332b10b",
   "metadata": {
    "papermill": {
     "duration": 0.015872,
     "end_time": "2025-01-10T13:54:14.994523",
     "exception": false,
     "start_time": "2025-01-10T13:54:14.978651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Setting up the dataset for modeling by splitting and scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402a25a",
   "metadata": {
    "papermill": {
     "duration": 0.029513,
     "end_time": "2025-01-10T13:54:15.040286",
     "exception": false,
     "start_time": "2025-01-10T13:54:15.010773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0, stratify=target)  # Import function to split dataset into training and testing sets\n",
    "\n",
    "# Save the split data\n",
    "print('-' * 10, 'DATA SPLITS', '-' * 10)   \n",
    "print('Training Features Shape:', X_train.shape)   \n",
    "print('Test Features Shape:', X_test.shape)   \n",
    "print('Training Target Shape:', y_train.shape)   \n",
    "print('Test Target Shape:', y_test.shape)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cfb0a0",
   "metadata": {
    "papermill": {
     "duration": 0.015135,
     "end_time": "2025-01-10T13:54:15.071262",
     "exception": false,
     "start_time": "2025-01-10T13:54:15.056127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb81cf",
   "metadata": {
    "papermill": {
     "duration": 0.017123,
     "end_time": "2025-01-10T13:54:15.103848",
     "exception": false,
     "start_time": "2025-01-10T13:54:15.086725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since we're making a classification task it's understandable that only the X features get to be scaled. Our target variable is also categorial which me that the scaling of features y are not applicable.\n",
    "To ensure consistency, the X_test also gets to be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013402e",
   "metadata": {
    "papermill": {
     "duration": 0.068854,
     "end_time": "2025-01-10T13:54:15.188447",
     "exception": false,
     "start_time": "2025-01-10T13:54:15.119593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()  # Import StandardScaler to normalize features\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)  \n",
    "\n",
    "# Save the scaled data\n",
    "pd.DataFrame(X_train_scaled, columns=X_train.columns).to_csv('X_train_scaled.csv', index=False)  \n",
    "pd.DataFrame(X_test_scaled, columns=X_test.columns).to_csv('X_test_scaled.csv', index=False)  \n",
    "\n",
    "X_train.to_csv('X_train.csv', index=False)  \n",
    "X_test.to_csv('X_test.csv', index=False)  \n",
    "y_train.to_csv('y_train.csv', index=False)  \n",
    "y_test.to_csv('y_test.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517dc50",
   "metadata": {
    "papermill": {
     "duration": 0.014971,
     "end_time": "2025-01-10T13:54:15.218953",
     "exception": false,
     "start_time": "2025-01-10T13:54:15.203982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ef0f6",
   "metadata": {
    "papermill": {
     "duration": 0.014868,
     "end_time": "2025-01-10T13:54:15.248921",
     "exception": false,
     "start_time": "2025-01-10T13:54:15.234053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Identify the most relevant features to improve the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63773aa",
   "metadata": {
    "papermill": {
     "duration": 0.042648,
     "end_time": "2025-01-10T13:54:15.306831",
     "exception": false,
     "start_time": "2025-01-10T13:54:15.264183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "X_train_scaled = pd.read_csv('X_train_scaled.csv')  \n",
    "X_train = pd.read_csv('X_train.csv')  \n",
    "y_train = pd.read_csv('y_train.csv')  \n",
    "\n",
    "X_test_scaled = pd.read_csv('X_test_scaled.csv')  \n",
    "X_test = pd.read_csv('X_test.csv')  \n",
    "y_test = pd.read_csv('y_test.csv')  \n",
    "\n",
    "print('Data loaded')   \n",
    "print('X_train_scaled shape:', X_train_scaled.shape)   \n",
    "print('X_train shape:', X_train.shape)   \n",
    "print('y_train shape:', y_train.shape)   \n",
    "print('X_test_scaled shape:', X_test_scaled.shape)   \n",
    "print('X_test shape:', X_test.shape)   \n",
    "print('y_test shape:', y_test.shape)   \n",
    "\n",
    "y_train = y_train.values.ravel()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc519a97",
   "metadata": {
    "papermill": {
     "duration": 0.015811,
     "end_time": "2025-01-10T13:54:15.338735",
     "exception": false,
     "start_time": "2025-01-10T13:54:15.322924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f0150",
   "metadata": {
    "papermill": {
     "duration": 44.933919,
     "end_time": "2025-01-10T13:55:00.288213",
     "exception": false,
     "start_time": "2025-01-10T13:54:15.354294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train, n_estimators=100, top_n_features=10):  \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, verbose=1, n_jobs=-1)  # Import RandomForestClassifier for building a Random Forest model\n",
    "    rf.fit(X_train, y_train)  \n",
    "    \n",
    "    feature_importance = pd.Series(rf.feature_importances_, index=X_train.columns)  \n",
    "    important_features = feature_importance.nlargest(top_n_features).index  \n",
    "    \n",
    "    feature_importance.nlargest(top_n_features).plot(kind='barh', title='Feature Importance')  \n",
    "    plt.show()  \n",
    "    \n",
    "    return rf, important_features  \n",
    "\n",
    "def perform_grid_search(X_train, y_train, param_grid):  \n",
    "    rf = RandomForestClassifier(random_state=42, n_jobs=-1)  # Import RandomForestClassifier for building a Random Forest model\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)  # Import GridSearchCV for hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train)  \n",
    "    \n",
    "    print(\"Best Parameters:\", grid_search.best_params_)   \n",
    "    print(\"Best Score:\", grid_search.best_score_)   \n",
    "    \n",
    "    return grid_search.best_estimator_  \n",
    "\n",
    "param_grid = {  \n",
    "    'n_estimators': [100, 200, 300],  \n",
    "    'max_depth': [10, 20, 30, 40, 50],  \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "}  \n",
    "\n",
    "rf, important_features = train_random_forest(X_train_scaled, y_train, top_n_features=10)  \n",
    "\n",
    "top_X_train = X_train_scaled[important_features]  \n",
    "top_X_test = X_test_scaled[important_features]  \n",
    "\n",
    "best_model = perform_grid_search(top_X_train, y_train, param_grid)  \n",
    "\n",
    "y_pred = best_model.predict(top_X_test)  \n",
    "y_pred_proba = best_model.predict_proba(top_X_test)[:, 1]  \n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)  # Import confusion_matrix to show prediction errors\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')  \n",
    "plt.xlabel('Predicted')  \n",
    "plt.ylabel('Actual')  \n",
    "plt.title('Confusion Matrix')  \n",
    "plt.show()  \n",
    "\n",
    "# Load the model\n",
    "filename = 'best_model.sav'  \n",
    "pickle.dump(best_model, open(filename, 'wb'))  \n",
    "\n",
    "# Load the model\n",
    "loaded_model = pickle.load(open(filename, 'rb'))  \n",
    "result = loaded_model.score(top_X_test, y_test)  \n",
    "print(result)   \n",
    "\n",
    "print('*'*10 + 'Results' + '*'*10)   \n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))   \n",
    "print('Precision:', precision_score(y_test, y_pred))   \n",
    "print('Recall:', recall_score(y_test, y_pred))   \n",
    "print('F1 Score:', f1_score(y_test, y_pred))   \n",
    "print('ROC AUC Score:', roc_auc_score(y_test, y_pred_proba))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f73f09",
   "metadata": {
    "papermill": {
     "duration": 0.017073,
     "end_time": "2025-01-10T13:55:00.323406",
     "exception": false,
     "start_time": "2025-01-10T13:55:00.306333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222346f",
   "metadata": {
    "papermill": {
     "duration": 5.356124,
     "end_time": "2025-01-10T13:55:05.696955",
     "exception": false,
     "start_time": "2025-01-10T13:55:00.340831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000)  # Import MLPClassifier for building a Multi-Layer Perceptron model\n",
    "mlp.fit(top_X_train, y_train)  \n",
    "\n",
    "# Cross-validation score\n",
    "print('*' * 10 + ' Cross Validation Score ' + '*' * 10)   \n",
    "print(cross_val_score(mlp, top_X_train, y_train, cv=5, scoring='accuracy').mean())   \n",
    "\n",
    "# Define Hyperparameter Grid for MLP\n",
    "param_grid_mlp = {  \n",
    "    'hidden_layer_sizes': [(100,), (200,), (300,)],  \n",
    "    'activation': ['logistic', 'tanh', 'relu'],  \n",
    "}  \n",
    "\n",
    "# Results\n",
    "print('*' * 10 + ' Results ' + '*' * 10)   \n",
    "print('Accuracy:', round(accuracy_score(y_test, mlp.predict(top_X_test)), 3))   \n",
    "print('Precision:', round(precision_score(y_test, mlp.predict(top_X_test)), 3))   \n",
    "print('Recall:', round(recall_score(y_test, mlp.predict(top_X_test)), 3))   \n",
    "print('F1 Score:', round(f1_score(y_test, mlp.predict(top_X_test)), 3))   \n",
    "print('ROC AUC Score:', round(roc_auc_score(y_test, mlp.predict_proba(top_X_test)[:, 1]), 3))   \n",
    "\n",
    "# Load the model\n",
    "# Save the trained MLP model\n",
    "filename = 'mlp_model.sav'  \n",
    "pickle.dump(mlp, open(filename, 'wb'))  \n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))  \n",
    "result = loaded_model.score(top_X_test, y_test)  \n",
    "print(result)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dcd570",
   "metadata": {
    "papermill": {
     "duration": 0.040601,
     "end_time": "2025-01-10T13:55:05.770411",
     "exception": false,
     "start_time": "2025-01-10T13:55:05.729810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e306d",
   "metadata": {
    "papermill": {
     "duration": 4.494996,
     "end_time": "2025-01-10T13:55:10.301965",
     "exception": false,
     "start_time": "2025-01-10T13:55:05.806969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_shape):  \n",
    "    model = Sequential([  # Import Sequential model class from Keras for building neural networks\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),  # Import Conv1D layer for 1D convolution (useful for sequential data like time series)\n",
    "        MaxPooling1D(pool_size=2),  # Import MaxPooling1D for downsampling 1D data\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),  # Import Conv1D layer for 1D convolution (useful for sequential data like time series)\n",
    "        MaxPooling1D(pool_size=2),  # Import MaxPooling1D for downsampling 1D data\n",
    "        Flatten(),  # Import Flatten layer to convert multi-dimensional input to 1D\n",
    "        Dense(128, activation='relu'),  # Import Dense (fully connected) layer\n",
    "        Dropout(0.5),  # Import Dropout for regularization to prevent overfitting\n",
    "        Dense(1, activation='sigmoid')  # Import Dense (fully connected) layer\n",
    "    ])  \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  \n",
    "    return model  \n",
    "\n",
    "# Reshape data for CNN (assuming time series-like data with 1D structure)\n",
    "top_X_train_cnn = top_X_train.values.reshape((top_X_train.shape[0], top_X_train.shape[1], 1))  \n",
    "top_X_test_cnn = top_X_test.values.reshape((top_X_test.shape[0], top_X_test.shape[1], 1))  \n",
    "\n",
    "# Initialize and train CNN model\n",
    "cnn_model = build_cnn(input_shape=(top_X_train.shape[1], 1))  \n",
    "history = cnn_model.fit(top_X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.2)  \n",
    "\n",
    "# Save CNN model\n",
    "cnn_model.save('cnn_model.h5')  \n",
    "\n",
    "# Results\n",
    "cnn_predictions = (cnn_model.predict(top_X_test_cnn) > 0.5).astype(\"int32\")  \n",
    "print('Results')   \n",
    "print('Accuracy:', accuracy_score(y_test, cnn_predictions))   \n",
    "print('Precision:', precision_score(y_test, cnn_predictions))   \n",
    "print('Recall:', recall_score(y_test, cnn_predictions))   \n",
    "print('F1 Score:', f1_score(y_test, cnn_predictions))   \n",
    "print('ROC AUC Score:', roc_auc_score(y_test, cnn_model.predict(top_X_test_cnn)))   \n",
    "\n",
    "# Plot Results\n",
    "fpr, tpr, thresholds = roc_curve(y_test, cnn_model.predict(top_X_test_cnn))  # Import roc_curve to compute Receiver Operating Characteristic curve\n",
    "roc_auc = auc(fpr, tpr)  # Import auc to compute Area Under Curve\n",
    "\n",
    "plt.figure()  \n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)  # Import auc to compute Area Under Curve\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  \n",
    "plt.xlim([0.0, 1.0])  \n",
    "plt.ylim([0.0, 1.05])  \n",
    "plt.xlabel('False Positive Rate')  \n",
    "plt.ylabel('True Positive Rate')  \n",
    "plt.title('Receiver Operating Characteristic (ROC)')  \n",
    "plt.legend(loc='lower right')  \n",
    "plt.show()  \n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, cnn_model.predict(top_X_test_cnn))  # Import precision_recall_curve to compute precision-recall tradeoff\n",
    "\n",
    "plt.figure()  \n",
    "plt.plot(recall, precision, color='darkorange', lw=2, label='Precision-Recall curve')  \n",
    "plt.xlim([0.0, 1.0])  \n",
    "plt.ylim([0.0, 1.05])  \n",
    "plt.xlabel('Recall')  \n",
    "plt.ylabel('Precision')  \n",
    "plt.title('Precision-Recall Curve')  \n",
    "plt.legend(loc='lower left')  \n",
    "plt.show()  \n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, cnn_predictions)  # Import confusion_matrix to show prediction errors\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')  \n",
    "plt.xlabel('Predicted')  \n",
    "plt.ylabel('Actual')  \n",
    "plt.title('Confusion Matrix')  \n",
    "plt.show()  \n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure()  \n",
    "plt.plot(history.history['accuracy'], label='accuracy')  \n",
    "plt.plot(history.history['loss'], label='loss')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.ylabel('Loss/Accuracy')  \n",
    "plt.title('Loss and Accuracy')  \n",
    "plt.legend()  \n",
    "plt.show()  \n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1303049,
     "sourceId": 2260912,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tokensim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 189.773666,
   "end_time": "2025-01-10T13:55:13.971326",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-10T13:52:04.197660",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
