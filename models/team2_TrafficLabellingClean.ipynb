{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "team2_TrafficLabellingClean.py\n",
    "-------------------------------\n",
    "Functions:\n",
    "- Safe CSV reader (utf-8-sig with latin1 fallback)\n",
    "- Cleaning: dropna, duplicates, constant cols, IP/Timestamp, extreme values\n",
    "- Feature selection: low variance + high correlation filter\n",
    "- Numeric optimization: downcast + rounding\n",
    "- Saves cleaned CSV & validation report\n",
    "- Logs run in MLflow\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Paths ----------------\n",
    "DATA_FOLDER = r\"C:\\Users\\hi\\AI-CloudSec-System\\data\\traffic\"\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "OUT_DIR = os.path.join(PROJECT_DIR, \"Step1-Datasets-Feature-Engineering\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_FILE = os.path.join(OUT_DIR, \"team2_TrafficLabellingClean.csv\")\n",
    "REPORT = os.path.join(OUT_DIR, \"team2_TrafficLabellingClean_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------------- MLflow Tracking ----------------\n",
    "mlflow.set_tracking_uri(f\"file:///{os.path.join(PROJECT_DIR, 'mlruns')}\")\n",
    "mlflow.set_experiment(\"Team2_Feature_Engineering_Traffic_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------------- Safe CSV Reader ----------------\n",
    "def safe_read_csv(path):\n",
    "    try:\n",
    "        return pd.read_csv(path, low_memory=False, encoding=\"utf-8-sig\")\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, low_memory=False, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------------- Cleaning ----------------\n",
    "def clean_dataframe(df, log):\n",
    "    before = len(df)\n",
    "    df = df.dropna().drop_duplicates()\n",
    "    log.append(f\"Dropna + duplicates: {before - len(df)} rows removed, now {len(df)} rows\")\n",
    "    \n",
    "    const_cols = df.columns[df.nunique() <= 1].tolist()\n",
    "    if const_cols:\n",
    "        df = df.drop(columns=const_cols)\n",
    "        log.append(f\"Dropped {len(const_cols)} constant cols: {const_cols}\")\n",
    "    \n",
    "    drop_cols = [c for c in df.columns if \"IP\" in c or \"Timestamp\" in c]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "        log.append(f\"Dropped {len(drop_cols)} IP/Timestamp cols\")\n",
    "    \n",
    "    if \" Flow Duration\" in df.columns:\n",
    "        df = df[(df[\" Flow Duration\"] > 0) & (df[\" Flow Duration\"] < 3600)]\n",
    "    if \" Flow Bytes/s\" in df.columns:\n",
    "        df = df[df[\" Flow Bytes/s\"] < 1e9]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------------- Feature Selection ----------------\n",
    "def feature_selection(df, log, label_col=\" Label\"):\n",
    "    if label_col in df.columns:\n",
    "        X = df.drop(columns=[label_col], errors=\"ignore\")\n",
    "    else:\n",
    "        X = df\n",
    "    X = X.select_dtypes(include=np.number)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    X_var = selector.fit_transform(X_scaled)\n",
    "    kept_cols = X.columns[selector.get_support()]\n",
    "    log.append(f\"Low variance removed: {X.shape[1] - len(kept_cols)} cols\")\n",
    "    \n",
    "    X_df = pd.DataFrame(X_var, columns=kept_cols)\n",
    "    corr = X_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "    if to_drop:\n",
    "        X_df = X_df.drop(columns=to_drop)\n",
    "        log.append(f\"High correlation removed: {len(to_drop)} cols\")\n",
    "    \n",
    "    if label_col in df.columns:\n",
    "        X_df[label_col] = df[label_col].values\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------------- Numeric Optimization ----------------\n",
    "def optimize_numeric(df, log, decimals=2):\n",
    "    before_mem = df.memory_usage(deep=True).sum() / (1024*1024)\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if pd.api.types.is_integer_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "        else:\n",
    "            df[col] = df[col].round(decimals)\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "    after_mem = df.memory_usage(deep=True).sum() / (1024*1024)\n",
    "    ratio = (before_mem - after_mem) / before_mem * 100\n",
    "    log.append(f\"Optimized numeric cols: {before_mem:.2f}MB â†’ {after_mem:.2f}MB (â†“{ratio:.1f}%)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------------- Main ----------------\n",
    "def main():\n",
    "    log = []\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.set_tag(\"step\", \"data_cleaning_and_feature_selection\")\n",
    "        files = [os.path.join(DATA_FOLDER, f) for f in os.listdir(DATA_FOLDER) if f.endswith(\".csv\")]\n",
    "        dfs = [safe_read_csv(f) for f in files]\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        log.append(f\"Merged {len(files)} files: {df.shape}\")\n",
    "        \n",
    "        df = clean_dataframe(df, log)\n",
    "        df_final = feature_selection(df, log)\n",
    "        df_final = optimize_numeric(df_final, log)\n",
    "        \n",
    "        mlflow.log_metric(\"final_rows\", len(df_final))\n",
    "        mlflow.log_metric(\"final_columns\", df_final.shape[1])\n",
    "        df_final.to_csv(OUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "        with open(REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(str(x) for x in log))\n",
    "        \n",
    "        mlflow.log_artifact(OUT_FILE, artifact_path=\"cleaned_data\")\n",
    "        mlflow.log_artifact(REPORT, artifact_path=\"reports\")\n",
    "        print(\"âœ… Saved cleaned dataset:\", OUT_FILE, df_final.shape)\n",
    "        print(\"ðŸ“Š Validation report written:\", REPORT)\n",
    "        print(f\"MLflow Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
