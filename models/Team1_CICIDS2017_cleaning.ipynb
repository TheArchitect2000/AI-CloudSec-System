{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6dffd9",
   "metadata": {},
   "source": [
    "# CIC-IDS2017 Cleaning Pipeline (with column drops)\n",
    "This notebook discovers CSV files, loads them robustly, applies a cleaning pipeline , and drop explicit columns, then saves the cleaned outputs to a custom folder.\n",
    "\n",
    "**For Team who Develop Model:**\n",
    "1. Code for x,y split is availabel in the last part.\n",
    "2. Download the cleaned dataset file, and apply the path for loading dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "127f61c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/xiao/Desktop/CSCI 7783/Group 4 Project/AI-CloudSec-System/AI-CloudSec-System/models\n",
      "INPUT_DIR = /Users/xiao/Desktop/CSCI 7783/Group 4 Project/AI-CloudSec-System/AI-CloudSec-System/models\n",
      "OUT_DIR   = /Users/xiao/Desktop/CSCI 7783/Group 4 Project/AI-CloudSec-System/AI-CloudSec-System/models/artifacts_cicids2017_clean_fe\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, glob, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configure paths ---\n",
    "INPUT_DIR = Path('.')  # change to your Kaggle input folder if needed\n",
    "OUT_DIR   = Path('artifacts_cicids2017_clean_fe')\n",
    "KEEP_FRACS = {'BENIGN': 1/3}\n",
    "FLOAT_DECIMALS = 5\n",
    "\n",
    "# Ensure output directory exists\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Working directory:', os.getcwd())\n",
    "print('INPUT_DIR =', INPUT_DIR.resolve())\n",
    "print('OUT_DIR   =', OUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c571a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 CSV file(s).\n",
      " - datasets/BotNeTIoT-L01_label_NoDuplicates.csv\n",
      " - datasets/ton-iot.csv\n",
      " - datasets/traffic_merged_clean.csv\n",
      " - datasets/traffic_merged_cleaned_strict_noconstant.csv\n"
     ]
    }
   ],
   "source": [
    "# Recursively discover CSV files under INPUT_DIR\n",
    "pattern = str(INPUT_DIR / '**/*.csv')\n",
    "dspaths = sorted(glob.glob(pattern, recursive=True))\n",
    "\n",
    "print(f'Found {len(dspaths)} CSV file(s).')\n",
    "for p in dspaths[:5]:\n",
    "    print(' -', p)\n",
    "\n",
    "assert dspaths, 'No CSVs found – check INPUT_DIR and filename pattern.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75067463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 DataFrame(s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version https://git-lfs.github.com/spec/v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oid sha256:26a5787b5fcd350ac4f669242c5c28e840c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>size 635529374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          version https://git-lfs.github.com/spec/v1\n",
       "0  oid sha256:26a5787b5fcd350ac4f669242c5c28e840c...\n",
       "1                                     size 635529374"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_float_precision(df, decimals=4):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(num_cols):\n",
    "        df[num_cols] = df[num_cols].round(decimals)\n",
    "    return df\n",
    "\n",
    "def safe_read_csv(path, **kwargs):\n",
    "    \"\"\"Robust CSV reader with fallbacks for encoding and separators.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, **kwargs)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding='latin1', **{k:v for k,v in kwargs.items() if k!='encoding'})\n",
    "    except pd.errors.ParserError:\n",
    "        for sep in [',', ';', '\\t', '|']:\n",
    "            try:\n",
    "                return pd.read_csv(path, sep=sep, **{k:v for k,v in kwargs.items() if k!='sep'})\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise\n",
    "\n",
    "individual_dfs = []\n",
    "for p in dspaths:\n",
    "    try:\n",
    "        df = safe_read_csv(p)\n",
    "        individual_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f'[skip] {p}: {e}')\n",
    "\n",
    "print(f'Loaded {len(individual_dfs)} DataFrame(s).')\n",
    "assert individual_dfs, 'No dataframes loaded – check file accessibility and formats.'\n",
    "\n",
    "individual_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86a32584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['id',\n",
       "  'Flow ID',\n",
       "  'Source IP',\n",
       "  'Src IP',\n",
       "  'Source Port',\n",
       "  'Src Port',\n",
       "  'Destination IP',\n",
       "  'Dst IP',\n",
       "  'Timestamp',\n",
       "  'Attempted Category'],\n",
       " ['id',\n",
       "  'Flow_ID',\n",
       "  'Source_IP',\n",
       "  'Src_IP',\n",
       "  'Source_Port',\n",
       "  'Src_Port',\n",
       "  'Destination_IP',\n",
       "  'Dst_IP'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to be dropped (original/raw names as they may appear in CIC-IDS2017 CSVs)\n",
    "drop_columns = [\n",
    "    \"id\",\n",
    "    \"Flow ID\",\n",
    "    \"Source IP\", \"Src IP\",\n",
    "    \"Source Port\", \"Src Port\",\n",
    "    \"Destination IP\", \"Dst IP\",\n",
    "    \"Timestamp\",\n",
    "    \"Attempted Category\",\n",
    "]\n",
    "\n",
    "def _normalize_name(name: str) -> str:\n",
    "    \"\"\"Normalize a column name the same way as our cleaning step.\"\"\"\n",
    "    name = name.strip()\n",
    "    name = re.sub(r'\\s+', '_', name)\n",
    "    name = re.sub(r'[^0-9a-zA-Z_]', '', name)\n",
    "    return name\n",
    "\n",
    "# Pre-compute a normalized drop list to match post-renaming columns\n",
    "drop_columns_normalized = [_normalize_name(c) for c in drop_columns]\n",
    "drop_columns, drop_columns_normalized[:8]  # sanity peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a01794f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "cnt = Counter()\n",
    "for df in individual_dfs:\n",
    "    if 'Label' in df.columns:\n",
    "        cnt.update(df['Label'].dropna().astype(str))\n",
    "\n",
    "pd.Series(cnt).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58283b02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c94302cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/qg/qls94sjx4mbcvyc5_jz0p2tr0000gn/T/ipykernel_56563/3783401218.py:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  .str.replace('\\s+', '_', regex=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version_httpsgitlfsgithubcomspecv1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oid sha256:26a5787b5fcd350ac4f669242c5c28e840c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>size 635529374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  version_httpsgitlfsgithubcomspecv1\n",
       "0  oid sha256:26a5787b5fcd350ac4f669242c5c28e840c...\n",
       "1                                     size 635529374"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cleaning pipeline \n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Drop columns using original/raw names (before renaming), if present\n",
    "    to_drop_raw = [c for c in df.columns if c in set(drop_columns)]\n",
    "    if to_drop_raw:\n",
    "        print('Dropping raw columns:', to_drop_raw)\n",
    "        df = df.drop(columns=to_drop_raw, errors='ignore')\n",
    "\n",
    "    # 2) Standardize column names\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace('\\s+', '_', regex=True)\n",
    "        .str.replace('[^0-9a-zA-Z_]', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # 3) Drop columns using normalized names (after renaming), if present\n",
    "    to_drop_norm = [c for c in df.columns if c in set(drop_columns_normalized)]\n",
    "    if to_drop_norm:\n",
    "        print('Dropping normalized columns:', to_drop_norm)\n",
    "        df = df.drop(columns=to_drop_norm, errors='ignore')\n",
    "\n",
    "    # 4) Replace infinite values with NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 5) Downcast numerics\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "    # 6) Drop duplicate rows\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after = len(df)\n",
    "    if before != after:\n",
    "        print(f'Dropped {before - after} duplicate row(s).')\n",
    "\n",
    "    # 7) Drop columns with too many NaNs\n",
    "    na_ratio = df.isna().mean()\n",
    "    drop_cols = na_ratio[na_ratio > 0.95].index.tolist()\n",
    "    if drop_cols:\n",
    "        print('Dropping high-NA columns (>95% NA):', drop_cols)\n",
    "        df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # 8) Fill NaNs (simple strategy)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    obj_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "    if len(num_cols) > 0:\n",
    "        df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "    if len(obj_cols) > 0:\n",
    "        df[obj_cols] = df[obj_cols].fillna('')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Test on one DF\n",
    "_test = clean_df(individual_dfs[0])\n",
    "_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c47c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame 1/4...\n",
      "  [skip] Missing columns: {'Bwd_Packet_Length_Min', 'Dst_Port', 'Total_Length_of_Bwd_Packet', 'Packet_Length_Max', 'Fwd_Packet_Length_Max', 'Total_Length_of_Fwd_Packet', 'Label', 'Total_Fwd_Packet', 'Total_Bwd_packets', 'Packet_Length_Variance', 'Bwd_Packet_Length_Std', 'Packet_Length_Min', 'Flow_Duration'}\n",
      "Processing DataFrame 2/4...\n",
      "  [skip] Missing columns: {'Bwd_Packet_Length_Min', 'Dst_Port', 'Total_Length_of_Bwd_Packet', 'Packet_Length_Max', 'Fwd_Packet_Length_Max', 'Total_Length_of_Fwd_Packet', 'Label', 'Total_Fwd_Packet', 'Total_Bwd_packets', 'Packet_Length_Variance', 'Bwd_Packet_Length_Std', 'Packet_Length_Min', 'Flow_Duration'}\n",
      "Processing DataFrame 3/4...\n",
      "  [skip] Missing columns: {'Bwd_Packet_Length_Min', 'Dst_Port', 'Total_Length_of_Bwd_Packet', 'Packet_Length_Max', 'Fwd_Packet_Length_Max', 'Total_Length_of_Fwd_Packet', 'Label', 'Total_Fwd_Packet', 'Total_Bwd_packets', 'Packet_Length_Variance', 'Bwd_Packet_Length_Std', 'Packet_Length_Min', 'Flow_Duration'}\n",
      "Processing DataFrame 4/4...\n",
      "  [skip] Missing columns: {'Bwd_Packet_Length_Min', 'Dst_Port', 'Total_Length_of_Bwd_Packet', 'Packet_Length_Max', 'Fwd_Packet_Length_Max', 'Total_Length_of_Fwd_Packet', 'Label', 'Total_Fwd_Packet', 'Total_Bwd_packets', 'Packet_Length_Variance', 'Bwd_Packet_Length_Std', 'Packet_Length_Min', 'Flow_Duration'}\n",
      "Total valid DataFrames to merge: 0\n"
     ]
    }
   ],
   "source": [
    "# --- keep exactly these 12 features (+ Label) ---\n",
    "FEATURES_12 = [\n",
    "    \"Dst_Port\",\n",
    "    \"Flow_Duration\",\n",
    "    \"Total_Fwd_Packet\",\n",
    "    \"Total_Bwd_packets\",\n",
    "    \"Total_Length_of_Fwd_Packet\",\n",
    "    \"Total_Length_of_Bwd_Packet\",\n",
    "    \"Packet_Length_Variance\",\n",
    "    \"Bwd_Packet_Length_Std\",\n",
    "    \"Packet_Length_Max\",\n",
    "    \"Packet_Length_Min\",\n",
    "    \"Bwd_Packet_Length_Min\",\n",
    "    \"Fwd_Packet_Length_Max\",]\n",
    "KEEP = set(FEATURES_12 + [\"Label\"])\n",
    "\n",
    "# -------- 1) read each file keeping only 12 cols (+Label) and MERGE -> cleaned.csv --------\n",
    "all_dfs = []\n",
    "for i, df in enumerate(individual_dfs):\n",
    "    print(f'Processing DataFrame {i+1}/{len(individual_dfs)}...')\n",
    "    df = clean_df(df)\n",
    "    missing = KEEP - set(df.columns)\n",
    "    if missing:\n",
    "        print(f'  [skip] Missing columns: {missing}')\n",
    "        continue\n",
    "    df = df[list(KEEP)]  # keep only desired columns\n",
    "    all_dfs.append(df)\n",
    "    print(f'  -> shape: {df.shape}')\n",
    "print(f'Total valid DataFrames to merge: {len(all_dfs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a3cad",
   "metadata": {},
   "source": [
    "Choose the features,merge and then compress the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3361e5f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#merge all, save as cleaned.csv\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m merged_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mMerged DataFrame shape:\u001b[39m\u001b[33m'\u001b[39m, merged_df.shape)\n\u001b[32m      4\u001b[39m merged_df = reduce_float_precision(merged_df, decimals=FLOAT_DECIMALS)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/core/reshape/concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/core/reshape/concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/core/reshape/concat.py:507\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    504\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    510\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "#merge all, save as cleaned.csv\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print('Merged DataFrame shape:', merged_df.shape)\n",
    "merged_df = reduce_float_precision(merged_df, decimals=FLOAT_DECIMALS)\n",
    "outpath_cleaned = OUT_DIR / 'cicids2017_cleaned.csv'\n",
    "merged_df.to_csv(outpath_cleaned, index=False)\n",
    "\n",
    "#compressed version\n",
    "outpath_cleaned_gz = OUT_DIR / 'cicids2017_cleaned.csv.zip'\n",
    "merged_df.to_csv(outpath_cleaned_gz, index=False, compression='zip')\n",
    "print('Saved cleaned data to:', outpath_cleaned)\n",
    "print('Saved compressed cleaned data to:', outpath_cleaned_gz)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d905647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping raw columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Timestamp', 'Attempted Category']\n",
      "Dropped 20691 duplicate row(s).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Clean one of the individual DataFrames to define \"cleaned\"\n",
    "cleaned = clean_df(individual_dfs[0])\n",
    "\n",
    "# Extract features, selecting specific columns\n",
    "x = cleaned.drop([\"Label\"], axis=1)\n",
    "x = x[['Flow_Duration','Dst_Port','Total_Length_of_Fwd_Packet','Total_Length_of_Bwd_Packet','Total_Fwd_Packet','Total_Bwd_packets','Active_Max','Active_Min','Fwd_Packet_Length_Max']]\n",
    "\n",
    "y = cleaned['Label'].values\n",
    "ss = StandardScaler()\n",
    "x = ss.fit_transform(x)  # Standardize the features\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da4574",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Discovered CSVs under `INPUT_DIR` (recursive).\n",
    "- Loaded data robustly with encoding/sep fallbacks.\n",
    "- Applied a cleaning pipeline with **explicit drops** for identifiers and PII-like columns:\n",
    "  - `id`, `Flow ID`, `Source IP`/`Src IP`, `Source Port`/`Src Port`, `Destination IP`/`Dst IP`, `Destination Port`/`Dst Port`, `Timestamp`, `Attempted Category`\n",
    "- Wrote cleaned CSVs into `OUT_DIR` with `_cleaned.csv` suffix.\n",
    "- (Optional) Produced a merged CSV for convenience.\n",
    "\n",
    "> Column dropping is resilient to both raw names **and** normalized names.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
