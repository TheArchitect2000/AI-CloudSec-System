{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6dffd9",
   "metadata": {},
   "source": [
    "# CIC-IDS2017 Cleaning Pipeline (with column drops)\n",
    "This notebook discovers CSV files, loads them robustly, applies a cleaning pipeline , and drop explicit columns, then saves the cleaned outputs to a custom folder.\n",
    "\n",
    "**For Team who Develop Model:**\n",
    "1. Code for x,y split is availabel in the last part.\n",
    "2. Download the cleaned dataset file, and apply the path for loading dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "127f61c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/jasmine/Documents/CSCI7783_Information Security/Project1_Data/CIC-IDS2017-00\n",
      "INPUT_DIR = /Users/jasmine/Documents/CSCI7783_Information Security/Project1_Data/CIC-IDS2017-00\n",
      "OUT_DIR   = /Users/jasmine/Documents/CSCI7783_Information Security/Project1_Data/CIC-IDS2017-00/artifacts_cicids2017_clean_fe\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, glob, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configure paths ---\n",
    "INPUT_DIR = Path('.')  # change to your Kaggle input folder if needed\n",
    "OUT_DIR   = Path('artifacts_cicids2017_clean_fe')\n",
    "KEEP_FRACS = {'BENIGN': 1/3}\n",
    "FLOAT_DECIMALS = 5\n",
    "\n",
    "# Ensure output directory exists\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Working directory:', os.getcwd())\n",
    "print('INPUT_DIR =', INPUT_DIR.resolve())\n",
    "print('OUT_DIR   =', OUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c571a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 CSV file(s).\n",
      " - archive/Benign-Monday.csv\n",
      " - archive/Bruteforce-Tuesday.csv\n",
      " - archive/DoS-Wednesday.csv\n",
      " - archive/Infiltration-Webattacks-Thursday.csv\n",
      " - archive/Portscan-DDos-Botnet-Friday.csv\n"
     ]
    }
   ],
   "source": [
    "# Recursively discover CSV files under INPUT_DIR\n",
    "pattern = str(INPUT_DIR / '**/*.csv')\n",
    "dspaths = sorted(glob.glob(pattern, recursive=True))\n",
    "\n",
    "print(f'Found {len(dspaths)} CSV file(s).')\n",
    "for p in dspaths[:5]:\n",
    "    print(' -', p)\n",
    "\n",
    "assert dspaths, 'No CSVs found – check INPUT_DIR and filename pattern.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75067463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 DataFrame(s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Src IP</th>\n",
       "      <th>Src Port</th>\n",
       "      <th>Dst IP</th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packet</th>\n",
       "      <th>...</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>ICMP Code</th>\n",
       "      <th>ICMP Type</th>\n",
       "      <th>Total TCP Flow Time</th>\n",
       "      <th>Label</th>\n",
       "      <th>Attempted Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.6.0.1-8.0.6.4-0-0-0</td>\n",
       "      <td>8.6.0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0.6.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-03 11:56:34.157427</td>\n",
       "      <td>119719148</td>\n",
       "      <td>231</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12685486.0</td>\n",
       "      <td>5.296658e+06</td>\n",
       "      <td>20694308.0</td>\n",
       "      <td>6499982.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>192.168.10.9-192.168.10.3-123-123-17</td>\n",
       "      <td>192.168.10.9</td>\n",
       "      <td>123</td>\n",
       "      <td>192.168.10.3</td>\n",
       "      <td>123</td>\n",
       "      <td>17</td>\n",
       "      <td>2017-07-03 11:56:55.428911</td>\n",
       "      <td>65511209</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1506210.0</td>\n",
       "      <td>64004884.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>64004884.0</td>\n",
       "      <td>64004884.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>192.168.10.12-224.0.0.251-5353-5353-17</td>\n",
       "      <td>192.168.10.12</td>\n",
       "      <td>5353</td>\n",
       "      <td>224.0.0.251</td>\n",
       "      <td>5353</td>\n",
       "      <td>17</td>\n",
       "      <td>2017-07-03 11:57:21.057686</td>\n",
       "      <td>113976922</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>25498178.0</td>\n",
       "      <td>1.883305e+07</td>\n",
       "      <td>48523116.0</td>\n",
       "      <td>5463561.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>192.168.10.12-152.2.133.52-123-123-17</td>\n",
       "      <td>192.168.10.12</td>\n",
       "      <td>123</td>\n",
       "      <td>152.2.133.52</td>\n",
       "      <td>123</td>\n",
       "      <td>17</td>\n",
       "      <td>2017-07-03 11:57:31.568196</td>\n",
       "      <td>67037196</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>11034681.0</td>\n",
       "      <td>55956316.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>55956316.0</td>\n",
       "      <td>55956316.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>192.168.10.12-45.79.11.217-123-123-17</td>\n",
       "      <td>192.168.10.12</td>\n",
       "      <td>123</td>\n",
       "      <td>45.79.11.217</td>\n",
       "      <td>123</td>\n",
       "      <td>17</td>\n",
       "      <td>2017-07-03 11:57:30.571719</td>\n",
       "      <td>68045057</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>11043596.0</td>\n",
       "      <td>56943904.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>56943904.0</td>\n",
       "      <td>56943904.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                 Flow ID         Src IP  Src Port  \\\n",
       "0   1                   8.6.0.1-8.0.6.4-0-0-0        8.6.0.1         0   \n",
       "1   2    192.168.10.9-192.168.10.3-123-123-17   192.168.10.9       123   \n",
       "2   3  192.168.10.12-224.0.0.251-5353-5353-17  192.168.10.12      5353   \n",
       "3   4   192.168.10.12-152.2.133.52-123-123-17  192.168.10.12       123   \n",
       "4   5   192.168.10.12-45.79.11.217-123-123-17  192.168.10.12       123   \n",
       "\n",
       "         Dst IP  Dst Port  Protocol                   Timestamp  \\\n",
       "0       8.0.6.4         0         0  2017-07-03 11:56:34.157427   \n",
       "1  192.168.10.3       123        17  2017-07-03 11:56:55.428911   \n",
       "2   224.0.0.251      5353        17  2017-07-03 11:57:21.057686   \n",
       "3  152.2.133.52       123        17  2017-07-03 11:57:31.568196   \n",
       "4  45.79.11.217       123        17  2017-07-03 11:57:30.571719   \n",
       "\n",
       "   Flow Duration  Total Fwd Packet  ...  Active Min   Idle Mean      Idle Std  \\\n",
       "0      119719148               231  ...        17.0  12685486.0  5.296658e+06   \n",
       "1       65511209                 6  ...   1506210.0  64004884.0  0.000000e+00   \n",
       "2      113976922               267  ...        14.0  25498178.0  1.883305e+07   \n",
       "3       67037196                 8  ...  11034681.0  55956316.0  0.000000e+00   \n",
       "4       68045057                 8  ...  11043596.0  56943904.0  0.000000e+00   \n",
       "\n",
       "     Idle Max    Idle Min  ICMP Code  ICMP Type  Total TCP Flow Time   Label  \\\n",
       "0  20694308.0   6499982.0         -1         -1                    0  BENIGN   \n",
       "1  64004884.0  64004884.0         -1         -1                    0  BENIGN   \n",
       "2  48523116.0   5463561.0         -1         -1                    0  BENIGN   \n",
       "3  55956316.0  55956316.0         -1         -1                    0  BENIGN   \n",
       "4  56943904.0  56943904.0         -1         -1                    0  BENIGN   \n",
       "\n",
       "   Attempted Category  \n",
       "0                  -1  \n",
       "1                  -1  \n",
       "2                  -1  \n",
       "3                  -1  \n",
       "4                  -1  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_float_precision(df, decimals=4):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(num_cols):\n",
    "        df[num_cols] = df[num_cols].round(decimals)\n",
    "    return df\n",
    "\n",
    "def safe_read_csv(path, **kwargs):\n",
    "    \"\"\"Robust CSV reader with fallbacks for encoding and separators.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, **kwargs)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding='latin1', **{k:v for k,v in kwargs.items() if k!='encoding'})\n",
    "    except pd.errors.ParserError:\n",
    "        for sep in [',', ';', '\\t', '|']:\n",
    "            try:\n",
    "                return pd.read_csv(path, sep=sep, **{k:v for k,v in kwargs.items() if k!='sep'})\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise\n",
    "\n",
    "individual_dfs = []\n",
    "for p in dspaths:\n",
    "    try:\n",
    "        df = safe_read_csv(p)\n",
    "        individual_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f'[skip] {p}: {e}')\n",
    "\n",
    "print(f'Loaded {len(individual_dfs)} DataFrame(s).')\n",
    "assert individual_dfs, 'No dataframes loaded – check file accessibility and formats.'\n",
    "\n",
    "individual_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "86a32584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['id',\n",
       "  'Flow ID',\n",
       "  'Source IP',\n",
       "  'Src IP',\n",
       "  'Source Port',\n",
       "  'Src Port',\n",
       "  'Destination IP',\n",
       "  'Dst IP',\n",
       "  'Timestamp',\n",
       "  'Attempted Category'],\n",
       " ['id',\n",
       "  'Flow_ID',\n",
       "  'Source_IP',\n",
       "  'Src_IP',\n",
       "  'Source_Port',\n",
       "  'Src_Port',\n",
       "  'Destination_IP',\n",
       "  'Dst_IP'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to be dropped (original/raw names as they may appear in CIC-IDS2017 CSVs)\n",
    "drop_columns = [\n",
    "    \"id\",\n",
    "    \"Flow ID\",\n",
    "    \"Source IP\", \"Src IP\",\n",
    "    \"Source Port\", \"Src Port\",\n",
    "    \"Destination IP\", \"Dst IP\",\n",
    "    \"Timestamp\",\n",
    "    \"Attempted Category\",\n",
    "]\n",
    "\n",
    "def _normalize_name(name: str) -> str:\n",
    "    \"\"\"Normalize a column name the same way as our cleaning step.\"\"\"\n",
    "    name = name.strip()\n",
    "    name = re.sub(r'\\s+', '_', name)\n",
    "    name = re.sub(r'[^0-9a-zA-Z_]', '', name)\n",
    "    return name\n",
    "\n",
    "# Pre-compute a normalized drop list to match post-renaming columns\n",
    "drop_columns_normalized = [_normalize_name(c) for c in drop_columns]\n",
    "drop_columns, drop_columns_normalized[:8]  # sanity peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a01794f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BENIGN                                    1587280\n",
       "Portscan                                   159066\n",
       "DoS Hulk                                   158449\n",
       "DDoS                                        95144\n",
       "Infiltration - Portscan                     67072\n",
       "DoS GoldenEye                                7567\n",
       "Botnet - Attempted                           4067\n",
       "DoS Slowloris                                3998\n",
       "FTP-Patator                                  3972\n",
       "DoS Slowhttptest - Attempted                 3367\n",
       "SSH-Patator                                  2961\n",
       "DoS Slowhttptest                             1741\n",
       "DoS Slowloris - Attempted                    1708\n",
       "Web Attack - Brute Force - Attempted         1292\n",
       "Botnet                                        736\n",
       "Web Attack - XSS - Attempted                  655\n",
       "DoS Hulk - Attempted                          581\n",
       "DoS GoldenEye - Attempted                      80\n",
       "Web Attack - Brute Force                       73\n",
       "Infiltration - Attempted                       45\n",
       "Infiltration                                   36\n",
       "SSH-Patator - Attempted                        27\n",
       "Web Attack - XSS                               18\n",
       "Web Attack - SQL Injection                     13\n",
       "FTP-Patator - Attempted                        12\n",
       "Heartbleed                                     11\n",
       "Web Attack - SQL Injection - Attempted          5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "cnt = Counter()\n",
    "for df in individual_dfs:\n",
    "    if 'Label' in df.columns:\n",
    "        cnt.update(df['Label'].dropna().astype(str))\n",
    "\n",
    "pd.Series(cnt).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58283b02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c94302cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping raw columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Timestamp', 'Attempted Category']\n",
      "Dropped 20691 duplicate row(s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dst_Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flow_Duration</th>\n",
       "      <th>Total_Fwd_Packet</th>\n",
       "      <th>Total_Bwd_packets</th>\n",
       "      <th>Total_Length_of_Fwd_Packet</th>\n",
       "      <th>Total_Length_of_Bwd_Packet</th>\n",
       "      <th>Fwd_Packet_Length_Max</th>\n",
       "      <th>Fwd_Packet_Length_Min</th>\n",
       "      <th>Fwd_Packet_Length_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Active_Max</th>\n",
       "      <th>Active_Min</th>\n",
       "      <th>Idle_Mean</th>\n",
       "      <th>Idle_Std</th>\n",
       "      <th>Idle_Max</th>\n",
       "      <th>Idle_Min</th>\n",
       "      <th>ICMP_Code</th>\n",
       "      <th>ICMP_Type</th>\n",
       "      <th>Total_TCP_Flow_Time</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>119719148</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22509459.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12685486.0</td>\n",
       "      <td>5.296658e+06</td>\n",
       "      <td>20694308.0</td>\n",
       "      <td>6499982.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>17</td>\n",
       "      <td>65511209</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>288.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1506210.0</td>\n",
       "      <td>1506210.0</td>\n",
       "      <td>64004884.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>64004884.0</td>\n",
       "      <td>64004884.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5353</td>\n",
       "      <td>17</td>\n",
       "      <td>113976922</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>20447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>76.580521</td>\n",
       "      <td>...</td>\n",
       "      <td>10983883.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>25498178.0</td>\n",
       "      <td>1.883305e+07</td>\n",
       "      <td>48523116.0</td>\n",
       "      <td>5463561.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123</td>\n",
       "      <td>17</td>\n",
       "      <td>67037196</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>384.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11034681.0</td>\n",
       "      <td>11034681.0</td>\n",
       "      <td>55956316.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>55956316.0</td>\n",
       "      <td>55956316.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123</td>\n",
       "      <td>17</td>\n",
       "      <td>68045057</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>384.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11043596.0</td>\n",
       "      <td>11043596.0</td>\n",
       "      <td>56943904.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>56943904.0</td>\n",
       "      <td>56943904.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dst_Port  Protocol  Flow_Duration  Total_Fwd_Packet  Total_Bwd_packets  \\\n",
       "0         0         0      119719148               231                  0   \n",
       "1       123        17       65511209                 6                  6   \n",
       "2      5353        17      113976922               267                  0   \n",
       "3       123        17       67037196                 8                  8   \n",
       "4       123        17       68045057                 8                  8   \n",
       "\n",
       "   Total_Length_of_Fwd_Packet  Total_Length_of_Bwd_Packet  \\\n",
       "0                         0.0                         0.0   \n",
       "1                       288.0                       288.0   \n",
       "2                     20447.0                         0.0   \n",
       "3                       384.0                       384.0   \n",
       "4                       384.0                       384.0   \n",
       "\n",
       "   Fwd_Packet_Length_Max  Fwd_Packet_Length_Min  Fwd_Packet_Length_Mean  ...  \\\n",
       "0                    0.0                    0.0                0.000000  ...   \n",
       "1                   48.0                   48.0               48.000000  ...   \n",
       "2                  153.0                   37.0               76.580521  ...   \n",
       "3                   48.0                   48.0               48.000000  ...   \n",
       "4                   48.0                   48.0               48.000000  ...   \n",
       "\n",
       "   Active_Max  Active_Min   Idle_Mean      Idle_Std    Idle_Max    Idle_Min  \\\n",
       "0  22509459.0        17.0  12685486.0  5.296658e+06  20694308.0   6499982.0   \n",
       "1   1506210.0   1506210.0  64004884.0  0.000000e+00  64004884.0  64004884.0   \n",
       "2  10983883.0        14.0  25498178.0  1.883305e+07  48523116.0   5463561.0   \n",
       "3  11034681.0  11034681.0  55956316.0  0.000000e+00  55956316.0  55956316.0   \n",
       "4  11043596.0  11043596.0  56943904.0  0.000000e+00  56943904.0  56943904.0   \n",
       "\n",
       "   ICMP_Code  ICMP_Type  Total_TCP_Flow_Time   Label  \n",
       "0         -1         -1                    0  BENIGN  \n",
       "1         -1         -1                    0  BENIGN  \n",
       "2         -1         -1                    0  BENIGN  \n",
       "3         -1         -1                    0  BENIGN  \n",
       "4         -1         -1                    0  BENIGN  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cleaning pipeline \n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Drop columns using original/raw names (before renaming), if present\n",
    "    to_drop_raw = [c for c in df.columns if c in set(drop_columns)]\n",
    "    if to_drop_raw:\n",
    "        print('Dropping raw columns:', to_drop_raw)\n",
    "        df = df.drop(columns=to_drop_raw, errors='ignore')\n",
    "\n",
    "    # 2) Standardize column names\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace('\\s+', '_', regex=True)\n",
    "        .str.replace('[^0-9a-zA-Z_]', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # 3) Drop columns using normalized names (after renaming), if present\n",
    "    to_drop_norm = [c for c in df.columns if c in set(drop_columns_normalized)]\n",
    "    if to_drop_norm:\n",
    "        print('Dropping normalized columns:', to_drop_norm)\n",
    "        df = df.drop(columns=to_drop_norm, errors='ignore')\n",
    "\n",
    "    # 4) Replace infinite values with NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 5) Downcast numerics\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "    # 6) Drop duplicate rows\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after = len(df)\n",
    "    if before != after:\n",
    "        print(f'Dropped {before - after} duplicate row(s).')\n",
    "\n",
    "    # 7) Drop columns with too many NaNs\n",
    "    na_ratio = df.isna().mean()\n",
    "    drop_cols = na_ratio[na_ratio > 0.95].index.tolist()\n",
    "    if drop_cols:\n",
    "        print('Dropping high-NA columns (>95% NA):', drop_cols)\n",
    "        df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # 8) Fill NaNs (simple strategy)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    obj_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "    if len(num_cols) > 0:\n",
    "        df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "    if len(obj_cols) > 0:\n",
    "        df[obj_cols] = df[obj_cols].fillna('')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Test on one DF\n",
    "_test = clean_df(individual_dfs[0])\n",
    "_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b4c47c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame 1/6...\n",
      "Dropping raw columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Timestamp', 'Attempted Category']\n",
      "Dropped 20691 duplicate row(s).\n",
      "  -> shape: (350933, 13)\n",
      "Processing DataFrame 2/6...\n",
      "Dropping raw columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Timestamp', 'Attempted Category']\n",
      "Dropped 14705 duplicate row(s).\n",
      "  -> shape: (307373, 13)\n",
      "Processing DataFrame 3/6...\n",
      "Dropping raw columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Timestamp', 'Attempted Category']\n",
      "Dropped 18494 duplicate row(s).\n",
      "  -> shape: (478147, 13)\n",
      "Processing DataFrame 4/6...\n",
      "Dropping raw columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Timestamp', 'Attempted Category']\n",
      "Dropped 44976 duplicate row(s).\n",
      "  -> shape: (317100, 13)\n",
      "Processing DataFrame 5/6...\n",
      "Dropping raw columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Timestamp', 'Attempted Category']\n",
      "Dropped 87639 duplicate row(s).\n",
      "  -> shape: (459918, 13)\n",
      "Processing DataFrame 6/6...\n",
      "  [skip] Missing columns: {'Bwd_Packet_Length_Min', 'Packet_Length_Variance', 'Total_Fwd_Packet', 'Total_Bwd_packets', 'Total_Length_of_Fwd_Packet', 'Fwd_Packet_Length_Max', 'Total_Length_of_Bwd_Packet', 'Flow_Duration', 'Packet_Length_Min', 'Dst_Port', 'Label', 'Bwd_Packet_Length_Std', 'Packet_Length_Max'}\n",
      "Total valid DataFrames to merge: 5\n"
     ]
    }
   ],
   "source": [
    "# --- keep exactly these 12 features (+ Label) ---\n",
    "FEATURES_12 = [\n",
    "    \"Dst_Port\",\n",
    "    \"Flow_Duration\",\n",
    "    \"Total_Fwd_Packet\",\n",
    "    \"Total_Bwd_packets\",\n",
    "    \"Total_Length_of_Fwd_Packet\",\n",
    "    \"Total_Length_of_Bwd_Packet\",\n",
    "    \"Packet_Length_Variance\",\n",
    "    \"Bwd_Packet_Length_Std\",\n",
    "    \"Packet_Length_Max\",\n",
    "    \"Packet_Length_Min\",\n",
    "    \"Bwd_Packet_Length_Min\",\n",
    "    \"Fwd_Packet_Length_Max\",]\n",
    "KEEP = set(FEATURES_12 + [\"Label\"])\n",
    "\n",
    "# -------- 1) read each file keeping only 12 cols (+Label) and MERGE -> cleaned.csv --------\n",
    "all_dfs = []\n",
    "for i, df in enumerate(individual_dfs):\n",
    "    print(f'Processing DataFrame {i+1}/{len(individual_dfs)}...')\n",
    "    df = clean_df(df)\n",
    "    missing = KEEP - set(df.columns)\n",
    "    if missing:\n",
    "        print(f'  [skip] Missing columns: {missing}')\n",
    "        continue\n",
    "    df = df[list(KEEP)]  # keep only desired columns\n",
    "    all_dfs.append(df)\n",
    "    print(f'  -> shape: {df.shape}')\n",
    "print(f'Total valid DataFrames to merge: {len(all_dfs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a3cad",
   "metadata": {},
   "source": [
    "Choose the features,merge and then compress the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361e5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (1913471, 13)\n",
      "Saved cleaned data to: artifacts_cicids2017_clean_fe/cicids2017_cleaned.csv\n",
      "Saved compressed cleaned data to: artifacts_cicids2017_clean_fe/cicids2017_cleaned.csv.zip\n"
     ]
    }
   ],
   "source": [
    "#merge all, save as cleaned.csv\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print('Merged DataFrame shape:', merged_df.shape)\n",
    "merged_df = reduce_float_precision(merged_df, decimals=FLOAT_DECIMALS)\n",
    "outpath_cleaned = OUT_DIR / 'cicids2017_cleaned.csv'\n",
    "merged_df.to_csv(outpath_cleaned, index=False)\n",
    "\n",
    "#compressed version\n",
    "outpath_cleaned_gz = OUT_DIR / 'cicids2017_cleaned.csv.zip'\n",
    "merged_df.to_csv(outpath_cleaned_gz, index=False, compression='zip')\n",
    "print('Saved cleaned data to:', outpath_cleaned)\n",
    "print('Saved compressed cleaned data to:', outpath_cleaned_gz)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d905647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping raw columns: ['id', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Timestamp', 'Attempted Category']\n",
      "Dropped 20691 duplicate row(s).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Clean one of the individual DataFrames to define \"cleaned\"\n",
    "cleaned = clean_df(individual_dfs[0])\n",
    "\n",
    "# Extract features, selecting specific columns\n",
    "x = cleaned.drop([\"Label\"], axis=1)\n",
    "x = x[['Flow_Duration','Dst_Port','Total_Length_of_Fwd_Packet','Total_Length_of_Bwd_Packet','Total_Fwd_Packet','Total_Bwd_packets','Active_Max','Active_Min','Fwd_Packet_Length_Max']]\n",
    "\n",
    "y = cleaned['Label'].values\n",
    "ss = StandardScaler()\n",
    "x = ss.fit_transform(x)  # Standardize the features\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da4574",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Discovered CSVs under `INPUT_DIR` (recursive).\n",
    "- Loaded data robustly with encoding/sep fallbacks.\n",
    "- Applied a cleaning pipeline with **explicit drops** for identifiers and PII-like columns:\n",
    "  - `id`, `Flow ID`, `Source IP`/`Src IP`, `Source Port`/`Src Port`, `Destination IP`/`Dst IP`, `Destination Port`/`Dst Port`, `Timestamp`, `Attempted Category`\n",
    "- Wrote cleaned CSVs into `OUT_DIR` with `_cleaned.csv` suffix.\n",
    "- (Optional) Produced a merged CSV for convenience.\n",
    "\n",
    "> Column dropping is resilient to both raw names **and** normalized names.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
