{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6dffd9",
   "metadata": {},
   "source": [
    "# CIC-IDS2017 Cleaning Pipeline (with column drops)\n",
    "This notebook discovers CSV files, loads them robustly, applies a cleaning pipeline , and drop explicit columns, then saves the cleaned outputs to a custom folder.\n",
    "\n",
    "**For Team who Develop Model:**\n",
    "1. Code for x,y split is availabel in the last part.\n",
    "2. Download the cleaned dataset file, and apply the path for loading dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, glob, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configure paths ---\n",
    "INPUT_DIR = Path('.')  # change to your Kaggle input folder if needed\n",
    "OUT_DIR   = Path('artifacts_cicids2017_clean_fe')\n",
    "KEEP_FRACS = {'BENIGN': 1/3}\n",
    "FLOAT_DECIMALS = 5\n",
    "\n",
    "# Ensure output directory exists\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Working directory:', os.getcwd())\n",
    "print('INPUT_DIR =', INPUT_DIR.resolve())\n",
    "print('OUT_DIR   =', OUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c571a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively discover CSV files under INPUT_DIR\n",
    "pattern = str(INPUT_DIR / '**/*.csv')\n",
    "dspaths = sorted(glob.glob(pattern, recursive=True))\n",
    "\n",
    "print(f'Found {len(dspaths)} CSV file(s).')\n",
    "for p in dspaths[:5]:\n",
    "    print(' -', p)\n",
    "\n",
    "assert dspaths, 'No CSVs found – check INPUT_DIR and filename pattern.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75067463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_float_precision(df, decimals=4):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(num_cols):\n",
    "        df[num_cols] = df[num_cols].round(decimals)\n",
    "    return df\n",
    "\n",
    "def safe_read_csv(path, **kwargs):\n",
    "    \"\"\"Robust CSV reader with fallbacks for encoding and separators.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, **kwargs)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding='latin1', **{k:v for k,v in kwargs.items() if k!='encoding'})\n",
    "    except pd.errors.ParserError:\n",
    "        for sep in [',', ';', '\\t', '|']:\n",
    "            try:\n",
    "                return pd.read_csv(path, sep=sep, **{k:v for k,v in kwargs.items() if k!='sep'})\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise\n",
    "\n",
    "individual_dfs = []\n",
    "for p in dspaths:\n",
    "    try:\n",
    "        df = safe_read_csv(p)\n",
    "        individual_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f'[skip] {p}: {e}')\n",
    "\n",
    "print(f'Loaded {len(individual_dfs)} DataFrame(s).')\n",
    "assert individual_dfs, 'No dataframes loaded – check file accessibility and formats.'\n",
    "\n",
    "individual_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a32584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be dropped (original/raw names as they may appear in CIC-IDS2017 CSVs)\n",
    "drop_columns = [\n",
    "    \"id\",\n",
    "    \"Flow ID\",\n",
    "    \"Source IP\", \"Src IP\",\n",
    "    \"Source Port\", \"Src Port\",\n",
    "    \"Destination IP\", \"Dst IP\",\n",
    "    \"Timestamp\",\n",
    "    \"Attempted Category\",\n",
    "]\n",
    "\n",
    "def _normalize_name(name: str) -> str:\n",
    "    \"\"\"Normalize a column name the same way as our cleaning step.\"\"\"\n",
    "    name = name.strip()\n",
    "    name = re.sub(r'\\s+', '_', name)\n",
    "    name = re.sub(r'[^0-9a-zA-Z_]', '', name)\n",
    "    return name\n",
    "\n",
    "# Pre-compute a normalized drop list to match post-renaming columns\n",
    "drop_columns_normalized = [_normalize_name(c) for c in drop_columns]\n",
    "drop_columns, drop_columns_normalized[:8]  # sanity peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01794f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "cnt = Counter()\n",
    "for df in individual_dfs:\n",
    "    if 'Label' in df.columns:\n",
    "        cnt.update(df['Label'].dropna().astype(str))\n",
    "\n",
    "pd.Series(cnt).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58283b02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94302cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleaning pipeline \n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Drop columns using original/raw names (before renaming), if present\n",
    "    to_drop_raw = [c for c in df.columns if c in set(drop_columns)]\n",
    "    if to_drop_raw:\n",
    "        print('Dropping raw columns:', to_drop_raw)\n",
    "        df = df.drop(columns=to_drop_raw, errors='ignore')\n",
    "\n",
    "    # 2) Standardize column names\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace('\\s+', '_', regex=True)\n",
    "        .str.replace('[^0-9a-zA-Z_]', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # 3) Drop columns using normalized names (after renaming), if present\n",
    "    to_drop_norm = [c for c in df.columns if c in set(drop_columns_normalized)]\n",
    "    if to_drop_norm:\n",
    "        print('Dropping normalized columns:', to_drop_norm)\n",
    "        df = df.drop(columns=to_drop_norm, errors='ignore')\n",
    "\n",
    "    # 4) Replace infinite values with NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 5) Downcast numerics\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "    # 6) Drop duplicate rows\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after = len(df)\n",
    "    if before != after:\n",
    "        print(f'Dropped {before - after} duplicate row(s).')\n",
    "\n",
    "    # 7) Drop columns with too many NaNs\n",
    "    na_ratio = df.isna().mean()\n",
    "    drop_cols = na_ratio[na_ratio > 0.95].index.tolist()\n",
    "    if drop_cols:\n",
    "        print('Dropping high-NA columns (>95% NA):', drop_cols)\n",
    "        df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # 8) Fill NaNs (simple strategy)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    obj_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "    if len(num_cols) > 0:\n",
    "        df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "    if len(obj_cols) > 0:\n",
    "        df[obj_cols] = df[obj_cols].fillna('')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Test on one DF\n",
    "_test = clean_df(individual_dfs[0])\n",
    "_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c47c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- keep exactly these 12 features (+ Label) ---\n",
    "FEATURES_12 = [\n",
    "    \"Dst_Port\",\n",
    "    \"Flow_Duration\",\n",
    "    \"Total_Fwd_Packet\",\n",
    "    \"Total_Bwd_packets\",\n",
    "    \"Total_Length_of_Fwd_Packet\",\n",
    "    \"Total_Length_of_Bwd_Packet\",\n",
    "    \"Packet_Length_Variance\",\n",
    "    \"Bwd_Packet_Length_Std\",\n",
    "    \"Packet_Length_Max\",\n",
    "    \"Packet_Length_Min\",\n",
    "    \"Bwd_Packet_Length_Min\",\n",
    "    \"Fwd_Packet_Length_Max\",]\n",
    "KEEP = set(FEATURES_12 + [\"Label\"])\n",
    "\n",
    "# -------- 1) read each file keeping only 12 cols (+Label) and MERGE -> cleaned.csv --------\n",
    "all_dfs = []\n",
    "for i, df in enumerate(individual_dfs):\n",
    "    print(f'Processing DataFrame {i+1}/{len(individual_dfs)}...')\n",
    "    df = clean_df(df)\n",
    "    missing = KEEP - set(df.columns)\n",
    "    if missing:\n",
    "        print(f'  [skip] Missing columns: {missing}')\n",
    "        continue\n",
    "    df = df[list(KEEP)]  # keep only desired columns\n",
    "    all_dfs.append(df)\n",
    "    print(f'  -> shape: {df.shape}')\n",
    "print(f'Total valid DataFrames to merge: {len(all_dfs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a3cad",
   "metadata": {},
   "source": [
    "Choose the features,merge and then compress the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all, save as cleaned.csv\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print('Merged DataFrame shape:', merged_df.shape)\n",
    "merged_df = reduce_float_precision(merged_df, decimals=FLOAT_DECIMALS)\n",
    "outpath_cleaned = OUT_DIR / 'cicids2017_cleaned.csv'\n",
    "merged_df.to_csv(outpath_cleaned, index=False)\n",
    "\n",
    "#compressed version\n",
    "outpath_cleaned_gz = OUT_DIR / 'cicids2017_cleaned.csv.zip'\n",
    "merged_df.to_csv(outpath_cleaned_gz, index=False, compression='zip')\n",
    "print('Saved cleaned data to:', outpath_cleaned)\n",
    "print('Saved compressed cleaned data to:', outpath_cleaned_gz)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d905647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Clean one of the individual DataFrames to define \"cleaned\"\n",
    "cleaned = clean_df(individual_dfs[0])\n",
    "\n",
    "# Extract features, selecting specific columns\n",
    "x = cleaned.drop([\"Label\"], axis=1)\n",
    "x = x[['Flow_Duration','Dst_Port','Total_Length_of_Fwd_Packet','Total_Length_of_Bwd_Packet','Total_Fwd_Packet','Total_Bwd_packets','Active_Max','Active_Min','Fwd_Packet_Length_Max']]\n",
    "\n",
    "y = cleaned['Label'].values\n",
    "ss = StandardScaler()\n",
    "x = ss.fit_transform(x)  # Standardize the features\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da4574",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Discovered CSVs under `INPUT_DIR` (recursive).\n",
    "- Loaded data robustly with encoding/sep fallbacks.\n",
    "- Applied a cleaning pipeline with **explicit drops** for identifiers and PII-like columns:\n",
    "  - `id`, `Flow ID`, `Source IP`/`Src IP`, `Source Port`/`Src Port`, `Destination IP`/`Dst IP`, `Destination Port`/`Dst Port`, `Timestamp`, `Attempted Category`\n",
    "- Wrote cleaned CSVs into `OUT_DIR` with `_cleaned.csv` suffix.\n",
    "- (Optional) Produced a merged CSV for convenience.\n",
    "\n",
    "> Column dropping is resilient to both raw names **and** normalized names.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
