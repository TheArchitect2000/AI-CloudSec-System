{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# UNSW-NB15 Full Dataset Preparation (Local Execution)\n",
    "This notebook prepares the full UNSW-NB15 dataset from the 4 split parts and GT labels. It downsamples benign traffic, filters columns, and prepares the data for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Load and Combine Data Parts, add on the missing headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "COLUMNS = [\n",
    "    'id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate',\n",
    "    'sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit',\n",
    "    'swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean',\n",
    "    'trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm',\n",
    "    'ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login',\n",
    "    'ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports',\n",
    "    'attack_cat','label'\n",
    "]               # ← Column names from the dataset documentation, used as headers\n",
    "\n",
    "# CONFIG: Set the path to your dataset folder\n",
    "DATA_DIR = Path(\"/Users/jasmine/Documents/CSCI7783_Information Security/Project1_Data/NUSW-NB15/data\")  # ← Change this!\n",
    "csv_parts = [f\"NUSW-NB15_{i}.csv\" for i in range(1, 5)]\n",
    "\n",
    "# Load and combine CSV files with column names as headers\n",
    "df_list = [pd.read_csv(DATA_DIR / part, names=COLUMNS, header=0) for part in csv_parts]\n",
    "\n",
    "first_row = df_list[0].iloc[[0]]  # Keep the first row from the first part\n",
    "df_combined = pd.concat(df_list, ignore_index=True)\n",
    "df_combined = pd.concat([first_row, df_combined], ignore_index=True)\n",
    "print(\"length of columns:\", len(df_combined.columns))\n",
    "\n",
    "# Nicely formatted preview\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 0)  # auto-fit\n",
    "pd.set_option(\"display.width\", 0)\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "print(\"\\n[check] first 22 rows:\")\n",
    "display(df_combined.head(22))\n",
    "len(df_rows := df_combined)\n",
    "print(f\"\\n[info] total rows: {len(df_rows)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Keep Only Required Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_nan = df_combined.isna().any(axis=1)\n",
    "to_drop = df_combined[mask_nan].sample(frac=0.6, random_state=42).index\n",
    "df_combined = df_combined.drop(index=to_drop).reset_index(drop=True)\n",
    "\n",
    "df_combined = df_combined.fillna(\"Normal\")\n",
    "\n",
    "print(\"Remaining rows:\", len(df_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Save Cleaned Data to CSV. ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUTPUT_CSV = DATA_DIR / \"UNSW_NB15_cleaned.csv\"\n",
    "OUTPUT_ZIP = DATA_DIR / \"UNSW_NB15_cleaned.csv.zip\"\n",
    "\n",
    "df_combined.to_csv(OUTPUT_CSV, index=False)\n",
    "df_combined.to_csv(OUTPUT_ZIP, index=False, compression={\n",
    "    \"method\": \"zip\",\n",
    "    \"archive_name\": OUTPUT_CSV.name\n",
    "})\n",
    "\n",
    "print(\"[save] CSV ->\", OUTPUT_CSV, \"size:\", OUTPUT_CSV.stat().st_size, \"bytes\")\n",
    "print(\"[save] ZIP ->\", OUTPUT_ZIP, \"size:\", OUTPUT_ZIP.stat().st_size, \"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Define x and y, then Split into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = df_combined.copy()\n",
    "# Drop non-predictive columns\n",
    "df = df.drop(columns=['id', 'label'])\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['attack_cat'])\n",
    "y = df['attack_cat']\n",
    "# One-hot encode categorical features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Train/test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"| X_test:\", X_test.shape)\n",
    "print(\"y_train distribution:\", y_train.value_counts(normalize=True).round(3).to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
