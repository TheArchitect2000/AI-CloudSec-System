{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "team2_TrafficLabellingClean.py\n",
    "-------------------------------\n",
    "Functions:\n",
    "- Safe CSV reader (utf-8-sig with latin1 fallback).\n",
    "- Basic cleaning: dropna, duplicates, constant columns,\n",
    "  IP/Timestamp columns, extreme values.\n",
    "- Lightweight feature selection: low variance filter +\n",
    "  high correlation filter.\n",
    "- Numeric optimization: downcast + rounding to reduce file size.\n",
    "- Outputs both cleaned CSV and validation report directly into /datasets.\n",
    "\"\"\"\n",
    "import mlflow\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "# Source dataset folder (update this path if needed)\n",
    "DATA_FOLDER = r\"C:\\Users\\hi\\AI-CloudSec-System\\data\\traffic\"\n",
    "\n",
    "# ---------------- Corrected Path Definitions ----------------\n",
    "# When running in Jupyter, os.getcwd() gives the notebook directory. \n",
    "# We navigate up one level (os.pardir) to the project root (AI-CloudSec-System-1).\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) \n",
    "# The target output directory is Step1-Datasets-Feature-Engineering.\n",
    "OUT_DIR = os.path.join(PROJECT_DIR, \"Step1-Datasets-Feature-Engineering\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define the final output file paths\n",
    "OUT_FILE = os.path.join(OUT_DIR, \"team2_TrafficLabellingClean.csv\")\n",
    "REPORT = os.path.join(OUT_DIR, \"team2_TrafficLabellingClean_report.txt\")\n",
    "\n",
    "# ---------------- Safe CSV Reader ----------------\n",
    "def safe_read_csv(path):\n",
    "    try:\n",
    "        print(f\"Reading {path} with utf-8-sig ...\")\n",
    "        return pd.read_csv(path, low_memory=False, encoding=\"utf-8-sig\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"‚ö†Ô∏è UTF-8 failed for {path}, retrying with latin1 ...\")\n",
    "        return pd.read_csv(path, low_memory=False, encoding=\"latin1\")\n",
    "\n",
    "\n",
    "# ---------------- Cleaning ----------------\n",
    "def clean_dataframe(df, log):\n",
    "    before = len(df)\n",
    "    df = df.dropna().drop_duplicates()\n",
    "    log.append(\n",
    "        f\"Dropna + duplicates: {before - len(df)} rows removed, now {len(df)} rows\"\n",
    "    )\n",
    "\n",
    "    const_cols = df.columns[df.nunique() <= 1].tolist()\n",
    "    if const_cols:\n",
    "        df = df.drop(columns=const_cols)\n",
    "        log.append(f\"Dropped {len(const_cols)} constant cols: {const_cols}\")\n",
    "\n",
    "    drop_cols = [c for c in df.columns if \"IP\" in c or \"Timestamp\" in c]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "        log.append(f\"Dropped {len(drop_cols)} IP/Timestamp cols\")\n",
    "\n",
    "    if \" Flow Duration\" in df.columns:\n",
    "        df = df[(df[\" Flow Duration\"] > 0) & (df[\" Flow Duration\"] < 3600)]\n",
    "    if \" Flow Bytes/s\" in df.columns:\n",
    "        df = df[df[\" Flow Bytes/s\"] < 1e9]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------- Feature Selection ----------------\n",
    "def feature_selection(df, log, label_col=\" Label\"):\n",
    "    if label_col in df.columns:\n",
    "        X = df.drop(columns=[label_col], errors=\"ignore\")\n",
    "    else:\n",
    "        X = df\n",
    "\n",
    "    X = X.select_dtypes(include=np.number)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    X_var = selector.fit_transform(X_scaled)\n",
    "    kept_cols = X.columns[selector.get_support()]\n",
    "    log.append(f\"Low variance removed: {X.shape[1] - len(kept_cols)} cols\")\n",
    "\n",
    "    X_df = pd.DataFrame(X_var, columns=kept_cols)\n",
    "    corr = X_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "    if to_drop:\n",
    "        X_df = X_df.drop(columns=to_drop)\n",
    "        log.append(f\"High correlation removed: {len(to_drop)} cols\")\n",
    "\n",
    "    if label_col in df.columns:\n",
    "        X_df[label_col] = df[label_col].values\n",
    "\n",
    "    return X_df\n",
    "\n",
    "\n",
    "# ---------------- Numeric Optimization ----------------\n",
    "def optimize_numeric(df, log, decimals=2):\n",
    "    before_mem = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if pd.api.types.is_integer_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "        else:\n",
    "            df[col] = df[col].round(decimals)\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "    after_mem = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    ratio = (before_mem - after_mem) / before_mem * 100\n",
    "    log.append(\n",
    "        f\"Optimized numeric cols: {before_mem:.2f}MB ‚Üí\"\n",
    "        f\"{after_mem:.2f}MB (‚Üì{ratio:.1f}%)\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------- Main (MLflow Integrated) ----------------\n",
    "def main():\n",
    "    log = []\n",
    "\n",
    "    # 1. Set MLflow Lab Name\n",
    "    mlflow.set_experiment(\"Team2_Feature_Engineering_Traffic_Data\")\n",
    "\n",
    "    # 2. Run a new MLflow \n",
    "    with mlflow.start_run() as run:\n",
    "        # Log the type of operation/step\n",
    "        mlflow.set_tag(\"step\", \"data_cleaning_and_feature_selection\")\n",
    "\n",
    "        # --- [Start my origin code logical] ---\n",
    "        files = [\n",
    "            os.path.join(DATA_FOLDER, f)\n",
    "            for f in os.listdir(DATA_FOLDER)\n",
    "            if f.endswith(\".csv\")\n",
    "        ]\n",
    "        dfs = [safe_read_csv(f) for f in files]\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        log.append(f\"Merged {len(files)} files: {df.shape}\")\n",
    "\n",
    "        df = clean_dataframe(df, log)\n",
    "        df_final = feature_selection(df, log)\n",
    "        df_final = optimize_numeric(df_final, log, decimals=2)\n",
    "\n",
    "        # record key index\n",
    "        mlflow.log_metric(\"final_rows\", len(df_final))\n",
    "        mlflow.log_metric(\"final_columns\", df_final.shape[1])\n",
    "\n",
    "        # save printout(original code)\n",
    "        # Note: OUT_FILE & REPORT Patch should be capable to visit in Notebook\n",
    "        df_final.to_csv(OUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "        with open(REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(str(x) for x in log))\n",
    "\n",
    "        # 3. Register MLflow Artifacts files\n",
    "        mlflow.log_artifact(OUT_FILE, artifact_path=\"cleaned_data\")\n",
    "        mlflow.log_artifact(REPORT, artifact_path=\"reports\")\n",
    "\n",
    "        print(\"‚úÖ Saved cleaned dataset:\", OUT_FILE, df_final.shape)\n",
    "        print(\"üìä Validation report written:\", REPORT)\n",
    "        print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "        \n",
    "# ---------------- Excutive Function ----------------\n",
    "# In Notebook ÔºåJust Call main() \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
