{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# BoTNeTIoT-L01 Review\n",
    "____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw file\n",
    "file_path = \"../Step1-Datasets-Feature-Engineering/team11_BotNeTIoT-L01_label_NoDuplicates.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "if len(df.columns) == 1 and \"github.com\" in df.columns[0]:\n",
    "    raise ValueError(\"You have downloaded an LFS pointer, and not the actual file.\\n\" \\\n",
    "    \"Please download team11_BotNeTIoT-L01_label_NoDuplicates.csv directly from the git repo.\\n\" \\\n",
    "    \"Place this download into the 'Step1-Datasets-Feature-Engineering' folder and try again.\")\n",
    "\n",
    "df = df.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "\n",
    "print(\"Data shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Data Cleaning & Handling Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Check missing values and drop if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample dataset, and drop unnamed\n",
    "\n",
    "print(f\"Total null values: {df.isnull().sum().sum()}\\n\")\n",
    "\n",
    "# Drop duplicates if any\n",
    "df = df.drop_duplicates()\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "df = df.apply(lambda x : np.round(x, 5))\n",
    "\n",
    "df_attack = df[df['label'] == 0]   # attack\n",
    "df_benign = df[df['label'] == 1]   # benign\n",
    "\n",
    "\n",
    "df_attack_downsampled = resample(\n",
    "    df_attack,\n",
    "    replace=False,\n",
    "    n_samples=len(df_benign),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_balanced = pd.concat([df_attack_downsampled, df_benign])\n",
    "df_balanced = df_balanced.reset_index()\n",
    "print(f\"Total value count:\\n\\n{df_balanced['label'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize using min max\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(df_balanced.drop(columns=['label']))\n",
    "y = df_balanced['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of benign vs attack per device (if device column exists)\n",
    "if 'device' in df_balanced.columns:\n",
    "    device_profiles = df_balanced.groupby(['device', 'label']).size().unstack(fill_value=0)\n",
    "    device_profiles['benign_attack_ratio'] = device_profiles['Benign'] / (device_profiles.sum(axis=1))\n",
    "    print(device_profiles.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure X is DataFrame\n",
    "X_orig = pd.DataFrame(X)\n",
    "\n",
    "# Reset indices to avoid mismatch\n",
    "X = X_orig.reset_index(drop=True)\n",
    "y = pd.Series(y).reset_index(drop=True)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df_xy = pd.concat([X, y.rename(\"label\")], axis=1)\n",
    "\n",
    "# Drop rows with NaN across all columns (features + label)\n",
    "df_xy = df_xy.dropna()\n",
    "\n",
    "# Split back into X and y\n",
    "X_clean = df_xy.drop(columns=['label'])\n",
    "y_clean = df_xy['label']\n",
    "\n",
    "\n",
    "# Build pipeline for PCA\n",
    "pca_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  # handles any remaining NaN\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=2))\n",
    "])\n",
    "\n",
    "# Fit + transform\n",
    "X_pca = pca_pipeline.fit_transform(X_clean)\n",
    "\n",
    "# Plot PCA\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y_clean, palette=\"Set1\", alpha=0.6)\n",
    "plt.title(\"PCA Scatterplot (with NaN handling and aligned labels)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create heatmap\n",
    "plt.figure(figsize=(50,50))\n",
    "sns.heatmap(pd.DataFrame(X).corr(), cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.xticks(list(range(0,len(df_balanced.columns))),df_balanced.columns)\n",
    "plt.yticks(list(range(0,len(df_balanced.columns))),df_balanced.columns)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Summary Before vs. After Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print feature shapes\n",
    "print(\"Original Feature Shape:\", df.shape)\n",
    "print(\"Engineered Feature Shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features before\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features after\n",
    "print(pd.DataFrame(X).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "After feature engineering and normalization, the dataset contains 1,026,994 samples with 28 features.\n",
    "\n",
    "The mean of each feature is close to 0, and the standard deviation is approximately 1, confirming that z-score normalization was applied successfully.\n",
    "\n",
    "Minimum and maximum values typically range between -8 and +8, indicating that outliers exist but are bounded within a normalized scale.\n",
    "\n",
    "The quartiles (25%, 50%, 75%) are centered around 0, showing the data is well-distributed after scaling.\n",
    "\n",
    "Compared to the raw dataset , the engineered dataset ensures fair comparability across all traffic features, which is crucial for PCA and classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Export Train/Split files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split onto train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
